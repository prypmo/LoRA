{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fMI0JT_YuYF3"
      },
      "source": [
        "## Exercício: Modelo de Linguagem com auto-atenção, máscaras causais e LoRA\n",
        "\n",
        "##Priscila Marques de Oliveira\n",
        "##RA094312\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CXFdJz2KVeQw"
      },
      "source": [
        "## Dados\n",
        "\n",
        "Vamos usar o mesmo dataset do Machado de Assis.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/ethelbeluzzi/projetomachado"
      ],
      "metadata": {
        "id": "qP6lAMiNkGou",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dc9c4327-7d87-4447-fb4a-e780bcbbaa88"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'projetomachado' already exists and is not an empty directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "DATA_PATH = os.path.join(\"projetomachado\", \"textonormalizado1000.txt\")\n",
        "\n",
        "# A príncipio, não estamos limpando as linhas\n",
        "with open(DATA_PATH, \"r\") as data_file:\n",
        "    lines = [line for line in data_file]\n",
        "\n",
        "# É possível voltar a um texto monolítico juntando as linhas.\n",
        "full_data = ' '.join(lines)\n",
        "full_data[:1000]"
      ],
      "metadata": {
        "id": "skzZnQbkkeeC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "outputId": "d6b48139-9f22-4961-fdd8-8dcd4c745474"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'1\\n MINISTÉRIO DA CULTURA\\n Fundação Biblioteca Nacional\\n Departamento Nacional do Livro\\n A MÃO E A LUVA\\n Machado de Assis\\n I\\n O fim da carta\\n Mas que pretendes fazer agora?\\n Morrer.\\n Morrer? Que idéia! Deixate disso, Estêvão. Não se morre por tão pouco...\\n Morrese. Quem não padece estas dores não as pode avaliar. O golpe foi profundo, e o\\n meu coração é pusilânime; por mais aborrecível que pareça a idéia da morte, pior, muito pior do\\n que ela, é a de viver. Ah! tu não sabes o que isto é?\\n Sei: um namoro gorado...\\n Luís!\\n ... E se em cada caso de namoro gorado morresse um homem, tinha já diminuído muito o\\n gênero humano, e Malthus perderia o latim. Anda, sobe.\\n Estêvão meteu a mão nos cabelos com um gesto de angústia; Luís Alves sacudiu a cabeça\\n e sorriu. Achavamse os dois no corredor da casa de Luís Alves, à rua da Constituição,  que\\n então se chamava dos Ciganos;  então, isto é, em 1853, uma bagatela de vinte anos que lá vão,\\n levando talvez consigo as ilusões do leitor, e deixandolhe'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Dados já foram separados em linhas\n",
        "# Checar tamanho das linhas em caracteres, por curiosidade\n",
        "lines = []\n",
        "line_lens = []\n",
        "\n",
        "with open(DATA_PATH, \"r\") as data_file:\n",
        "    for line in data_file:\n",
        "        lines.append(line)\n",
        "        line_lens.append(len(line))\n"
      ],
      "metadata": {
        "id": "oY3HPglrkvqf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Limpar linhas, removendo \\n, espaços antes e depois\n",
        "with open(DATA_PATH, \"r\") as data_file:\n",
        "    cleaned_lines = [line.strip().lower() for line in data_file]\n",
        "\n",
        "len(cleaned_lines)"
      ],
      "metadata": {
        "id": "6MGoKUTFk0sK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "42928a39-025a-4977-c2e4-a59d5a63b0fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "306409"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sum([len(cleaned_line) for cleaned_line in cleaned_lines])"
      ],
      "metadata": {
        "id": "yLaexBQI5pNn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6de365b6-74cf-45bf-c24c-360b343ba05b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "18539036"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# É possível voltar a um texto monolítico juntando as linhas. Nota-se que estamos adicionando espaços, mas não há mais \\n\n",
        "full_data = ' '.join(cleaned_lines)\n",
        "len(full_data)"
      ],
      "metadata": {
        "id": "MyoFd6whtt2Q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c5ac78be-ee88-4ace-8cc1-9498fa28c846"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "18845444"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "full_data[:1000]"
      ],
      "metadata": {
        "id": "sfDqi_cjc9N0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "outputId": "62855a30-c7ac-4dbf-b7af-9a70f2b24521"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'1 ministério da cultura fundação biblioteca nacional departamento nacional do livro a mão e a luva machado de assis i o fim da carta mas que pretendes fazer agora? morrer. morrer? que idéia! deixate disso, estêvão. não se morre por tão pouco... morrese. quem não padece estas dores não as pode avaliar. o golpe foi profundo, e o meu coração é pusilânime; por mais aborrecível que pareça a idéia da morte, pior, muito pior do que ela, é a de viver. ah! tu não sabes o que isto é? sei: um namoro gorado... luís! ... e se em cada caso de namoro gorado morresse um homem, tinha já diminuído muito o gênero humano, e malthus perderia o latim. anda, sobe. estêvão meteu a mão nos cabelos com um gesto de angústia; luís alves sacudiu a cabeça e sorriu. achavamse os dois no corredor da casa de luís alves, à rua da constituição,  que então se chamava dos ciganos;  então, isto é, em 1853, uma bagatela de vinte anos que lá vão, levando talvez consigo as ilusões do leitor, e deixandolhe em troca usurários! '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Separar em treino e teste\n",
        "# Tamanhos das divisões\n",
        "train_limit = int(0.7 * len(cleaned_lines))\n",
        "val_limit = int(0.2 * len(cleaned_lines)) + train_limit\n",
        "\n",
        "# Dividindo os dados\n",
        "train_cleaned_lines = cleaned_lines[:train_limit]\n",
        "val_cleaned_lines = cleaned_lines[train_limit:val_limit]\n",
        "test_cleaned_lines = cleaned_lines[val_limit:]\n",
        "\n",
        "# Não utilize o split val para nada a partir daqui, somente validar"
      ],
      "metadata": {
        "id": "eQHK1Hgtd-2P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(train_cleaned_lines)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EPBduunOt3Cl",
        "outputId": "7e8efe1f-1907-4921-f965-6103d1ed0e6b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "214486"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(val_cleaned_lines)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jz7G3dbdt5Ir",
        "outputId": "a8ba375a-b26f-487d-ec83-3a4af749037c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "61281"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(test_cleaned_lines)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qdyhoL9Nt6zr",
        "outputId": "3c63a7bf-6069-4ce2-f482-3a2b5f56319d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "30642"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Pré Processamento"
      ],
      "metadata": {
        "id": "zmtW887IsuQe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt_tab')\n",
        "from nltk.tokenize import word_tokenize"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eFjQnMDCtM7W",
        "outputId": "b686afd0-8b6a-4d8f-92a8-631a434e1d3e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from nltk.tokenize import word_tokenize, RegexpTokenizer\n",
        "\n",
        "def preprocess_text(text, keep_punctuation=False):\n",
        "    \"\"\"\n",
        "    Pré-processamento para corpus de previsão da próxima palavra.\n",
        "    - lowercase\n",
        "    - normaliza espaços\n",
        "    - substituir números por <NUM>\n",
        "    \"\"\"\n",
        "    text = text.lower()\n",
        "\n",
        "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
        "\n",
        "    tokens = word_tokenize(text)\n",
        "\n",
        "    return tokens"
      ],
      "metadata": {
        "id": "qFdNlmcCtWOA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preprocess_trained_text = [preprocess_text(line) for line in train_cleaned_lines]"
      ],
      "metadata": {
        "id": "QCJSkKuatbV9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_cleaned_lines[5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "Bs1KPvTUywXS",
        "outputId": "756421b9-ce65-4d25-bd7b-c204b3e7b716"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'machado de assis'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "preprocess_trained_text[1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZTeQyDGgx76k",
        "outputId": "a7db4689-9ddc-4e35-d092-adf28d662abb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['ministério', 'da', 'cultura']"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(preprocess_trained_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yeF9MfBVV9pM",
        "outputId": "6287ab64-2f62-4d31-e600-6b41bacf109a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "214486"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "preprocess_val_text = [preprocess_text(line) for line in val_cleaned_lines]"
      ],
      "metadata": {
        "id": "nN0q6jh1x1cN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(preprocess_val_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2BzigtUmWAIg",
        "outputId": "d2e31b8b-0aab-4fb6-b999-d3bb511811b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "61281"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "preprocess_test_text = [preprocess_text(line) for line in test_cleaned_lines]"
      ],
      "metadata": {
        "id": "0mYYu1GY2VXy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(preprocess_test_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lDsGvbPV2WBk",
        "outputId": "f3da9211-1dc1-4287-eb74-5c9aaf7f5efb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "30642"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "import re\n",
        "# Contar número de palavras ÚNICAS\n",
        "def count_words(texts):\n",
        "    # Counter: collection especifica do Python para contar ocorrências de um objeto\n",
        "    word_counts = Counter()\n",
        "    for text in texts:\n",
        "        word_counts.update(re.findall(r'\\w+', text.lower()))\n",
        "        #### Separado com regex, \\w+: sequências alfanuméricas\n",
        "\n",
        "\n",
        "    return word_counts\n",
        "\n",
        "word_counts = count_words(train_cleaned_lines)\n"
      ],
      "metadata": {
        "id": "Yc2eOhQ9Objj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word_counts"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "NEH7EGBj0p0K",
        "outputId": "37b547b3-f87b-4fc3-873f-4bccf18a673b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Counter({'1': 211,\n",
              "         'ministério': 135,\n",
              "         'da': 23790,\n",
              "         'cultura': 24,\n",
              "         'fundação': 18,\n",
              "         'biblioteca': 165,\n",
              "         'nacional': 146,\n",
              "         'departamento': 11,\n",
              "         'do': 26770,\n",
              "         'livro': 946,\n",
              "         'a': 92380,\n",
              "         'mão': 1869,\n",
              "         'e': 70069,\n",
              "         'luva': 29,\n",
              "         'machado': 478,\n",
              "         'de': 75020,\n",
              "         'assis': 559,\n",
              "         'i': 764,\n",
              "         'o': 67182,\n",
              "         'fim': 1616,\n",
              "         'carta': 2032,\n",
              "         'mas': 16256,\n",
              "         'que': 82710,\n",
              "         'pretendes': 15,\n",
              "         'fazer': 1571,\n",
              "         'agora': 2950,\n",
              "         'morrer': 461,\n",
              "         'idéia': 1700,\n",
              "         'deixate': 19,\n",
              "         'disso': 505,\n",
              "         'estêvão': 452,\n",
              "         'não': 43624,\n",
              "         'se': 18280,\n",
              "         'morre': 101,\n",
              "         'por': 11537,\n",
              "         'tão': 3496,\n",
              "         'pouco': 3006,\n",
              "         'morrese': 7,\n",
              "         'quem': 2601,\n",
              "         'padece': 23,\n",
              "         'estas': 871,\n",
              "         'dores': 133,\n",
              "         'as': 15880,\n",
              "         'pode': 2343,\n",
              "         'avaliar': 28,\n",
              "         'golpe': 225,\n",
              "         'foi': 7566,\n",
              "         'profundo': 223,\n",
              "         'meu': 4746,\n",
              "         'coração': 2440,\n",
              "         'é': 20636,\n",
              "         'pusilânime': 5,\n",
              "         'mais': 10629,\n",
              "         'aborrecível': 9,\n",
              "         'pareça': 50,\n",
              "         'morte': 1093,\n",
              "         'pior': 340,\n",
              "         'muito': 4102,\n",
              "         'ela': 6367,\n",
              "         'viver': 440,\n",
              "         'ah': 903,\n",
              "         'tu': 1293,\n",
              "         'sabes': 290,\n",
              "         'isto': 2254,\n",
              "         'sei': 2007,\n",
              "         'um': 28216,\n",
              "         'namoro': 174,\n",
              "         'gorado': 2,\n",
              "         'luís': 1250,\n",
              "         'em': 16957,\n",
              "         'cada': 888,\n",
              "         'caso': 1454,\n",
              "         'morresse': 28,\n",
              "         'homem': 2661,\n",
              "         'tinha': 4190,\n",
              "         'já': 4935,\n",
              "         'diminuído': 10,\n",
              "         'gênero': 142,\n",
              "         'humano': 148,\n",
              "         'malthus': 1,\n",
              "         'perderia': 23,\n",
              "         'latim': 116,\n",
              "         'anda': 268,\n",
              "         'sobe': 50,\n",
              "         'meteu': 108,\n",
              "         'nos': 3111,\n",
              "         'cabelos': 376,\n",
              "         'com': 18707,\n",
              "         'gesto': 1026,\n",
              "         'angústia': 42,\n",
              "         'alves': 452,\n",
              "         'sacudiu': 53,\n",
              "         'cabeça': 1411,\n",
              "         'sorriu': 401,\n",
              "         'achavamse': 22,\n",
              "         'os': 20047,\n",
              "         'dois': 2237,\n",
              "         'no': 11396,\n",
              "         'corredor': 234,\n",
              "         'casa': 4929,\n",
              "         'à': 10852,\n",
              "         'rua': 1610,\n",
              "         'constituição': 56,\n",
              "         'então': 2739,\n",
              "         'chamava': 106,\n",
              "         'dos': 7293,\n",
              "         'ciganos': 12,\n",
              "         '1853': 4,\n",
              "         'uma': 18140,\n",
              "         'bagatela': 2,\n",
              "         'vinte': 714,\n",
              "         'anos': 2377,\n",
              "         'lá': 2548,\n",
              "         'vão': 615,\n",
              "         'levando': 78,\n",
              "         'talvez': 1746,\n",
              "         'consigo': 583,\n",
              "         'ilusões': 132,\n",
              "         'leitor': 660,\n",
              "         'deixandolhe': 7,\n",
              "         'troca': 123,\n",
              "         'usurários': 3,\n",
              "         'triste': 830,\n",
              "         'crua': 24,\n",
              "         'desconsolada': 10,\n",
              "         'experiência': 123,\n",
              "         'eram': 2378,\n",
              "         'nove': 164,\n",
              "         'horas': 1330,\n",
              "         'noite': 2053,\n",
              "         'recolhiase': 11,\n",
              "         'para': 16162,\n",
              "         'justamente': 415,\n",
              "         'na': 10125,\n",
              "         'ocasião': 809,\n",
              "         'ia': 2445,\n",
              "         'procurar': 97,\n",
              "         'encontraramse': 25,\n",
              "         'porta': 1382,\n",
              "         'ali': 1440,\n",
              "         'mesmo': 3516,\n",
              "         'lhe': 9303,\n",
              "         'confiou': 58,\n",
              "         'tudo': 4404,\n",
              "         'havia': 1721,\n",
              "         'saberá': 38,\n",
              "         'daqui': 464,\n",
              "         'aborreça': 13,\n",
              "         'histórias': 82,\n",
              "         'amor': 2560,\n",
              "         'velhas': 179,\n",
              "         'como': 9732,\n",
              "         'adão': 37,\n",
              "         'eternas': 54,\n",
              "         'céu': 1086,\n",
              "         'amigos': 921,\n",
              "         'demoraramse': 2,\n",
              "         'ainda': 4127,\n",
              "         'algum': 1771,\n",
              "         'tempo': 4211,\n",
              "         'insistir': 22,\n",
              "         'outro': 3567,\n",
              "         'subisse': 8,\n",
              "         'teimar': 25,\n",
              "         'queria': 1005,\n",
              "         'ir': 1906,\n",
              "         'tenazes': 4,\n",
              "         'ambos': 1087,\n",
              "         'haveria': 85,\n",
              "         'meio': 1403,\n",
              "         'vencer': 115,\n",
              "         'ocorresse': 5,\n",
              "         'transação': 5,\n",
              "         'pois': 1380,\n",
              "         'sim': 1562,\n",
              "         'disse': 5587,\n",
              "         'ele': 9276,\n",
              "         'convenho': 15,\n",
              "         'deves': 66,\n",
              "         'há': 4624,\n",
              "         'ser': 4444,\n",
              "         'amanhã': 582,\n",
              "         'cede': 43,\n",
              "         'tua': 787,\n",
              "         'parte': 1431,\n",
              "         'vem': 585,\n",
              "         'passar': 482,\n",
              "         'comigo': 1037,\n",
              "         'nestas': 106,\n",
              "         'últimas': 196,\n",
              "         'tens': 311,\n",
              "         'terra': 917,\n",
              "         'darmeás': 1,\n",
              "         'lição': 171,\n",
              "         'eu': 10786,\n",
              "         'te': 1279,\n",
              "         'pagarei': 1,\n",
              "         'outra': 3709,\n",
              "         'filosofia': 178,\n",
              "         'dizendo': 638,\n",
              "         'travou': 40,\n",
              "         'braço': 329,\n",
              "         'resistiu': 74,\n",
              "         'dessa': 223,\n",
              "         'vez': 3164,\n",
              "         'ou': 7811,\n",
              "         'porque': 3126,\n",
              "         'houvesse': 293,\n",
              "         'entranhado': 9,\n",
              "         'deveras': 344,\n",
              "         'cé': 5,\n",
              "         'rebro': 1,\n",
              "         'cedesse': 13,\n",
              "         'ao': 13530,\n",
              "         'doloroso': 38,\n",
              "         'gosto': 821,\n",
              "         'falar': 994,\n",
              "         'mulher': 1625,\n",
              "         'amada': 111,\n",
              "         'provável': 194,\n",
              "         'esses': 317,\n",
              "         'motivos': 133,\n",
              "         'juntos': 265,\n",
              "         'vamos': 600,\n",
              "         'nós': 832,\n",
              "         'eles': 1436,\n",
              "         'escada': 203,\n",
              "         'acima': 289,\n",
              "         'até': 3155,\n",
              "         'sala': 1125,\n",
              "         'visitas': 302,\n",
              "         'onde': 1872,\n",
              "         'beijar': 72,\n",
              "         'sua': 3281,\n",
              "         'mãe': 2189,\n",
              "         'mamãe': 408,\n",
              "         'fazerme': 48,\n",
              "         'favor': 290,\n",
              "         'mandar': 219,\n",
              "         'chá': 229,\n",
              "         'quarto': 559,\n",
              "         'passa': 207,\n",
              "         '2': 137,\n",
              "         'murmurou': 184,\n",
              "         'algumas': 1438,\n",
              "         'palavras': 1813,\n",
              "         'tentou': 72,\n",
              "         'dar': 1272,\n",
              "         'ar': 993,\n",
              "         'gracejo': 54,\n",
              "         'fúnebres': 20,\n",
              "         'cipreste': 5,\n",
              "         'viulhe': 15,\n",
              "         'luz': 577,\n",
              "         'das': 5252,\n",
              "         'estearinas': 1,\n",
              "         'alguma': 2676,\n",
              "         'vermelhidão': 2,\n",
              "         'olhos': 4173,\n",
              "         'adivinhou': 70,\n",
              "         'era': 15434,\n",
              "         'difícil': 343,\n",
              "         'chorado': 11,\n",
              "         'pobre': 639,\n",
              "         'rapaz': 1277,\n",
              "         'suspirou': 111,\n",
              "         'mentalmente': 69,\n",
              "         'dali': 394,\n",
              "         'foram': 873,\n",
              "         'vasta': 72,\n",
              "         'três': 1704,\n",
              "         'camas': 15,\n",
              "         'cadeiras': 74,\n",
              "         'todos': 2548,\n",
              "         'feitios': 6,\n",
              "         'duas': 2652,\n",
              "         'estantes': 22,\n",
              "         'livros': 395,\n",
              "         'secretária': 44,\n",
              "         'vindo': 152,\n",
              "         'alcova': 159,\n",
              "         'gabinete': 272,\n",
              "         'estudo': 174,\n",
              "         'subiu': 128,\n",
              "         'daí': 725,\n",
              "         'rogo': 3,\n",
              "         'hóspede': 68,\n",
              "         'bebeu': 28,\n",
              "         'goles': 23,\n",
              "         'acendeu': 75,\n",
              "         'cigarro': 40,\n",
              "         'entrou': 920,\n",
              "         'passear': 118,\n",
              "         'longo': 364,\n",
              "         'aposento': 36,\n",
              "         'enquanto': 668,\n",
              "         'preferindo': 14,\n",
              "         'charuto': 149,\n",
              "         'so': 119,\n",
              "         'fá': 2,\n",
              "         'primeiro': 1455,\n",
              "         'estirouse': 10,\n",
              "         'segundo': 582,\n",
              "         'cruzando': 4,\n",
              "         'beatificamente': 2,\n",
              "         'mãos': 1410,\n",
              "         'sobre': 1292,\n",
              "         'ventre': 55,\n",
              "         'contemplando': 25,\n",
              "         'bico': 36,\n",
              "         'chinelas': 26,\n",
              "         'aquela': 955,\n",
              "         'placidez': 40,\n",
              "         'gorou': 4,\n",
              "         'nenhum': 836,\n",
              "         'silêncio': 772,\n",
              "         'completo': 64,\n",
              "         'ouviase': 9,\n",
              "         'rodar': 20,\n",
              "         'carros': 66,\n",
              "         'passavam': 99,\n",
              "         'fora': 1561,\n",
              "         'porém': 948,\n",
              "         'único': 395,\n",
              "         'rumor': 131,\n",
              "         'botins': 7,\n",
              "         'palhinha': 14,\n",
              "         'chão': 538,\n",
              "         'cursavam': 4,\n",
              "         'estes': 443,\n",
              "         'moços': 69,\n",
              "         'academia': 182,\n",
              "         's': 1437,\n",
              "         'paulo': 903,\n",
              "         'estando': 165,\n",
              "         'ano': 677,\n",
              "         'terceiro': 232,\n",
              "         'conheceramse': 6,\n",
              "         'ficaram': 237,\n",
              "         'íntimos': 61,\n",
              "         'tanto': 1329,\n",
              "         'quanto': 826,\n",
              "         'podiam': 318,\n",
              "         'sêlo': 56,\n",
              "         'espíritos': 119,\n",
              "         'diferentes': 90,\n",
              "         'isso': 2541,\n",
              "         'dotado': 34,\n",
              "         'extrema': 33,\n",
              "         'sensibilidade': 35,\n",
              "         'menor': 265,\n",
              "         'fraqueza': 67,\n",
              "         'ânimo': 185,\n",
              "         'afetuoso': 27,\n",
              "         'bom': 1154,\n",
              "         'daquela': 492,\n",
              "         'bondade': 81,\n",
              "         'varonil': 6,\n",
              "         'apanágio': 5,\n",
              "         'alma': 1618,\n",
              "         'forte': 330,\n",
              "         'mole': 31,\n",
              "         'cera': 33,\n",
              "         'vai': 1371,\n",
              "         'mercê': 13,\n",
              "         'todas': 1685,\n",
              "         'circunstâncias': 265,\n",
              "         'além': 679,\n",
              "         'infortúnio': 50,\n",
              "         'trazer': 131,\n",
              "         'nariz': 335,\n",
              "         'óculos': 44,\n",
              "         'corderosa': 35,\n",
              "         'suas': 1542,\n",
              "         'virginais': 1,\n",
              "         'via': 518,\n",
              "         'bem': 2772,\n",
              "         'cara': 468,\n",
              "         'nã': 51,\n",
              "         'mau': 398,\n",
              "         'seu': 3341,\n",
              "         'grão': 21,\n",
              "         'egoísmo': 67,\n",
              "         'incapaz': 41,\n",
              "         'afeições': 73,\n",
              "         'sabia': 717,\n",
              "         'regêlas': 2,\n",
              "         'moderá': 1,\n",
              "         'las': 21,\n",
              "         'sobretudo': 219,\n",
              "         'guiálas': 2,\n",
              "         'próprio': 799,\n",
              "         'interesse': 410,\n",
              "         'entre': 2363,\n",
              "         'homens': 860,\n",
              "         'travarase': 5,\n",
              "         'amizade': 303,\n",
              "         'íntima': 107,\n",
              "         'nascida': 27,\n",
              "         'simpatia': 186,\n",
              "         'costume': 583,\n",
              "         'naturais': 89,\n",
              "         'confidentes': 9,\n",
              "         'diferença': 326,\n",
              "         'dava': 620,\n",
              "         'menos': 2584,\n",
              "         'recebia': 96,\n",
              "         'assim': 3036,\n",
              "         'nem': 5974,\n",
              "         'exprimia': 42,\n",
              "         'grande': 2171,\n",
              "         'confiança': 301,\n",
              "         'referira': 1,\n",
              "         'amigo': 1545,\n",
              "         'desde': 1081,\n",
              "         'tempos': 388,\n",
              "         'toda': 1494,\n",
              "         'história': 628,\n",
              "         'malogrado': 7,\n",
              "         'esperanças': 347,\n",
              "         'desalentos': 4,\n",
              "         'glórias': 50,\n",
              "         'enfim': 990,\n",
              "         'inesperado': 43,\n",
              "         'desfecho': 44,\n",
              "         'folheava': 14,\n",
              "         'capítulo': 2195,\n",
              "         'delicioso': 64,\n",
              "         'romance': 230,\n",
              "         'sentir': 211,\n",
              "         'dele': 1664,\n",
              "         'caiu': 196,\n",
              "         'altura': 72,\n",
              "         'dura': 84,\n",
              "         'prosaica': 3,\n",
              "         'miserável': 45,\n",
              "         'realidade': 340,\n",
              "         'namorada': 99,\n",
              "         'dizer': 2456,\n",
              "         'coisa': 2464,\n",
              "         'dela': 1432,\n",
              "         'moça': 2436,\n",
              "         '17': 56,\n",
              "         'ora': 1083,\n",
              "         'simples': 899,\n",
              "         'alunaprofessora': 1,\n",
              "         'colégio': 159,\n",
              "         'tia': 411,\n",
              "         'nosso': 864,\n",
              "         'estudante': 120,\n",
              "         'inválidos': 53,\n",
              "         'tinhaa': 22,\n",
              "         'visto': 416,\n",
              "         'pela': 2274,\n",
              "         'primeira': 1452,\n",
              "         'seis': 426,\n",
              "         'meses': 836,\n",
              "         'antes': 2120,\n",
              "         'd': 4073,\n",
              "         'esde': 3,\n",
              "         'logo': 2233,\n",
              "         'sentiuse': 56,\n",
              "         'preso': 90,\n",
              "         'referindolhe': 4,\n",
              "         'encontro': 136,\n",
              "         'fez': 1502,\n",
              "         'sorrir': 129,\n",
              "         'estirado': 27,\n",
              "         'prazo': 104,\n",
              "         'qualquer': 682,\n",
              "         'fosse': 2001,\n",
              "         'fatal': 151,\n",
              "         'daquele': 501,\n",
              "         'cativeiro': 28,\n",
              "         'verdade': 2193,\n",
              "         'ponto': 850,\n",
              "         'viu': 998,\n",
              "         'amou': 50,\n",
              "         'ama': 301,\n",
              "         'vida': 2816,\n",
              "         'estouvado': 16,\n",
              "         'cego': 45,\n",
              "         'sincero': 155,\n",
              "         'puro': 231,\n",
              "         'amavao': 16,\n",
              "         'dizia': 1356,\n",
              "         'devia': 814,\n",
              "         'crêlo': 17,\n",
              "         'alguns': 2325,\n",
              "         'olhares': 75,\n",
              "         'ternos': 20,\n",
              "         'meia': 482,\n",
              "         'dúzia': 91,\n",
              "         'apertos': 11,\n",
              "         'significativos': 1,\n",
              "         'embora': 487,\n",
              "         'largos': 59,\n",
              "         'intervalos': 37,\n",
              "         'davam': 186,\n",
              "         'entender': 201,\n",
              "         'guiomar': 286,\n",
              "         'chamavase': 105,\n",
              "         'surdo': 42,\n",
              "         'paixão': 525,\n",
              "         'acadêmico': 26,\n",
              "         'nada': 4139,\n",
              "         'flor': 658,\n",
              "         'colhida': 3,\n",
              "         'pé': 1217,\n",
              "         'original': 244,\n",
              "         'frescura': 36,\n",
              "         'murcha': 11,\n",
              "         'sem': 5071,\n",
              "         'cheiro': 38,\n",
              "         'dada': 117,\n",
              "         'senão': 505,\n",
              "         'pedida': 19,\n",
              "         'fazme': 29,\n",
              "         'dia': 3589,\n",
              "         'apontando': 36,\n",
              "         'trazia': 533,\n",
              "         'esta': 3364,\n",
              "         'está': 2309,\n",
              "         'naturalmente': 738,\n",
              "         'deitála': 1,\n",
              "         'despentearse': 3,\n",
              "         'desejava': 92,\n",
              "         'ma': 131,\n",
              "         'desse': 432,\n",
              "         'sorrindo': 324,\n",
              "         'tirou': 208,\n",
              "         'f': 107,\n",
              "         'lor': 2,\n",
              "         'cabelo': 86,\n",
              "         'deulha': 5,\n",
              "         'recebeua': 20,\n",
              "         'igual': 387,\n",
              "         'contentamento': 71,\n",
              "         'teria': 376,\n",
              "         'antecipassem': 1,\n",
              "         'quinhão': 11,\n",
              "         'suprir': 19,\n",
              "         'cartas': 625,\n",
              "         'obtivera': 7,\n",
              "         'durante': 770,\n",
              "         'aqueles': 169,\n",
              "         'compridos': 22,\n",
              "         'serem': 91,\n",
              "         'tais': 609,\n",
              "         'afinal': 369,\n",
              "         'são': 2186,\n",
              "         'vãose': 4,\n",
              "         'donde': 228,\n",
              "         'vieram': 330,\n",
              "         'aquilo': 363,\n",
              "         'capricho': 97,\n",
              "         'passatempo': 28,\n",
              "         '3': 96,\n",
              "         'naquela': 494,\n",
              "         'tarde': 1268,\n",
              "         'sós': 249,\n",
              "         'raro': 150,\n",
              "         'disselhe': 573,\n",
              "         'breve': 251,\n",
              "         'voltar': 345,\n",
              "         'imagem': 217,\n",
              "         'pedindolhe': 55,\n",
              "         'câmbio': 8,\n",
              "         'escrevesse': 31,\n",
              "         'franziu': 50,\n",
              "         'testa': 208,\n",
              "         'fitou': 52,\n",
              "         'nele': 407,\n",
              "         'magnífico': 95,\n",
              "         'par': 201,\n",
              "         'castanhos': 45,\n",
              "         'tanta': 259,\n",
              "         'irritação': 43,\n",
              "         'dignidade': 117,\n",
              "         'ficou': 1152,\n",
              "         'atônito': 32,\n",
              "         'perplexo': 12,\n",
              "         'imaginase': 12,\n",
              "         'diante': 667,\n",
              "         'reinou': 2,\n",
              "         'segundos': 113,\n",
              "         'imagina': 178,\n",
              "         'dor': 619,\n",
              "         'prostrou': 7,\n",
              "         'espanto': 209,\n",
              "         'quando': 5382,\n",
              "         'erguendose': 22,\n",
              "         'cadeira': 393,\n",
              "         'estava': 3192,\n",
              "         'respondeu': 1219,\n",
              "         'saindo': 105,\n",
              "         'esqueçase': 5,\n",
              "         'mim': 2403,\n",
              "         'ouvindo': 196,\n",
              "         'terceira': 167,\n",
              "         'esqueciame': 15,\n",
              "         'curarme': 5,\n",
              "         'cima': 393,\n",
              "         'compêndios': 5,\n",
              "         'direito': 397,\n",
              "         'romano': 24,\n",
              "         'conheço': 167,\n",
              "         'remédio': 286,\n",
              "         'melhor': 1979,\n",
              "         'achaques': 10,\n",
              "         'ouvia': 271,\n",
              "         'assentado': 43,\n",
              "         'cama': 399,\n",
              "         'cotovelos': 14,\n",
              "         'fincados': 6,\n",
              "         'nas': 1671,\n",
              "         'pernas': 207,\n",
              "         'metida': 57,\n",
              "         'parecendo': 57,\n",
              "         'chorava': 73,\n",
              "         'princípio': 605,\n",
              "         'chorou': 97,\n",
              "         'tardou': 109,\n",
              "         'visse': 151,\n",
              "         'deitarse': 14,\n",
              "         'estorcerse': 1,\n",
              "         'convulsivamente': 1,\n",
              "         'soluçar': 16,\n",
              "         'abafar': 16,\n",
              "         'podia': 2536,\n",
              "         'gritos': 51,\n",
              "         'saíam': 53,\n",
              "         'peito': 275,\n",
              "         'puxar': 22,\n",
              "         'pedir': 308,\n",
              "         'entremeado': 13,\n",
              "         'nome': 1158,\n",
              "         'dalma': 37,\n",
              "         'lastimosamente': 4,\n",
              "         'natural': 682,\n",
              "         'comoveu': 13,\n",
              "         'houve': 570,\n",
              "         'dizerlhe': 177,\n",
              "         'conforto': 31,\n",
              "         'consolação': 109,\n",
              "         'veio': 998,\n",
              "         'chegada': 111,\n",
              "         'paroxismo': 1,\n",
              "         'declinou': 1,\n",
              "         'apouco': 2,\n",
              "         'lágrimas': 706,\n",
              "         'estancaram': 1,\n",
              "         'parecerte': 1,\n",
              "         'ridículo': 92,\n",
              "         'sentandose': 77,\n",
              "         'qu': 103,\n",
              "         'queres': 236,\n",
              "         'vivia': 258,\n",
              "         'persuasão': 37,\n",
              "         'amado': 200,\n",
              "         'erao': 31,\n",
              "         'entendo': 104,\n",
              "         'passou': 568,\n",
              "         'hoje': 1288,\n",
              "         'supunha': 118,\n",
              "         'passava': 228,\n",
              "         'zombaria': 12,\n",
              "         'interrompeu': 276,\n",
              "         'compreendendo': 13,\n",
              "         'curar': 62,\n",
              "         'meterlhe': 5,\n",
              "         'brios': 17,\n",
              "         'amorpróprio': 56,\n",
              "         'instantes': 420,\n",
              "         'pensativo': 47,\n",
              "         'possível': 585,\n",
              "         'contestou': 16,\n",
              "         'conheces': 38,\n",
              "         'grave': 389,\n",
              "         'nobre': 208,\n",
              "         'criatura': 288,\n",
              "         'conceber': 14,\n",
              "         'sentimento': 590,\n",
              "         'desses': 196,\n",
              "         'seria': 1191,\n",
              "         'vulgar': 148,\n",
              "         'cruel': 161,\n",
              "         'mulheres': 379,\n",
              "         'pensei': 190,\n",
              "         'maneira': 399,\n",
              "         'experimentarme': 4,\n",
              "         'ver': 2218,\n",
              "         'escusas': 11,\n",
              "         'rirte': 1,\n",
              "         'afirmo': 52,\n",
              "         'digo': 819,\n",
              "         'admira': 121,\n",
              "         'fizesse': 221,\n",
              "         'esse': 1806,\n",
              "         'cálculo': 101,\n",
              "         'nesse': 573,\n",
              "         'todo': 1535,\n",
              "         'filho': 1257,\n",
              "         'imaginação': 301,\n",
              "         'desceu': 190,\n",
              "         'este': 3093,\n",
              "         'declívio': 1,\n",
              "         'floridas': 5,\n",
              "         'conjecturas': 12,\n",
              "         'entendeu': 138,\n",
              "         'aviso': 44,\n",
              "         'espantarlhe': 1,\n",
              "         'cavalos': 98,\n",
              "         'abaixo': 220,\n",
              "         'rédea': 53,\n",
              "         'froux': 1,\n",
              "         'riso': 299,\n",
              "         'lábios': 299,\n",
              "         'boa': 1186,\n",
              "         'viagem': 546,\n",
              "         'exclamou': 325,\n",
              "         'colega': 107,\n",
              "         'voltando': 117,\n",
              "         'estirarse': 3,\n",
              "         'sofá': 111,\n",
              "         'longa': 361,\n",
              "         'produziu': 76,\n",
              "         'efeito': 629,\n",
              "         'salutar': 18,\n",
              "         'namorado': 253,\n",
              "         'adoçandolhe': 1,\n",
              "         'penas': 40,\n",
              "         'circunstância': 190,\n",
              "         'aproveitou': 38,\n",
              "         'cem': 134,\n",
              "         'coisas': 1146,\n",
              "         'alheias': 70,\n",
              "         'divertilo': 4,\n",
              "         'pensamento': 375,\n",
              "         'absorvia': 2,\n",
              "         'conseguiu': 74,\n",
              "         'intento': 11,\n",
              "         'hora': 1059,\n",
              "         'risse': 11,\n",
              "         'amargo': 40,\n",
              "         'dúbio': 1,\n",
              "         'depois': 4731,\n",
              "         'jovial': 62,\n",
              "         'franco': 86,\n",
              "         'incompatível': 12,\n",
              "         'intuitos': 10,\n",
              "         'trágicos': 9,\n",
              "         'ai': 169,\n",
              "         'espécie': 387,\n",
              "         'tosse': 32,\n",
              "         'moral': 351,\n",
              "         'aplacava': 7,\n",
              "         'reaparecia': 4,\n",
              "         'intensa': 47,\n",
              "         'às': 2840,\n",
              "         'vezes': 1536,\n",
              "         'fraca': 37,\n",
              "         'sempre': 1554,\n",
              "         'infalível': 23,\n",
              "         'acertara': 11,\n",
              "         'abrir': 181,\n",
              "         'página': 290,\n",
              "         'werther': 2,\n",
              "         'leu': 299,\n",
              "         'linhas': 229,\n",
              "         'acesso': 40,\n",
              "         'voltou': 512,\n",
              "         'nunca': 1729,\n",
              "         'acudiulhe': 10,\n",
              "         'pastilhas': 9,\n",
              "         'nova': 835,\n",
              "         'palestra': 78,\n",
              "         'novo': 664,\n",
              "         'desespero': 191,\n",
              "         'escoando': 1,\n",
              "         'relógio': 248,\n",
              "         'jantar': 847,\n",
              "         'batia': 108,\n",
              "         'seca': 65,\n",
              "         'regular': 32,\n",
              "         'mente': 128,\n",
              "         'lembrar': 151,\n",
              "         'aos': 2788,\n",
              "         'nossas': 284,\n",
              "         'paixões': 136,\n",
              "         'aceleram': 1,\n",
              "         'moderam': 1,\n",
              "         'passo': 579,\n",
              "         'aurora': 134,\n",
              "         'acadêmicos': 13,\n",
              "         'coincidiu': 5,\n",
              "         'badaladas': 2,\n",
              "         'meiodia': 84,\n",
              "         'não4': 1,\n",
              "         'só': 4309,\n",
              "         'adormeceram': 1,\n",
              "         'começava': 180,\n",
              "         'apagar': 46,\n",
              "         'estrelas': 164,\n",
              "         'manhã': 804,\n",
              "         'quero': 776,\n",
              "         'sossegada': 34,\n",
              "         'livre': 232,\n",
              "         'sonhos': 228,\n",
              "         'maus': 77,\n",
              "         'abriu': 291,\n",
              "         'estranhou': 10,\n",
              "         'objetos': 93,\n",
              "         'rodeavam': 6,\n",
              "         'reconheceu': 89,\n",
              "         'despertouselhe': 1,\n",
              "         'memória': 465,\n",
              "         'aguda': 18,\n",
              "         'véspera': 392,\n",
              "         'sucessos': 119,\n",
              "         'recentes': 33,\n",
              "         'começavam': 33,\n",
              "         'envolverse': 1,\n",
              "         'sombra': 329,\n",
              "         'crepuscular': 4,\n",
              "         'passado': 618,\n",
              "         'natureza': 655,\n",
              "         'tem': 2110,\n",
              "         'leis': 151,\n",
              "         'imperiosas': 6,\n",
              "         'complexo': 15,\n",
              "         'vive': 144,\n",
              "         'também': 3748,\n",
              "         'força': 724,\n",
              "         'dizêlo': 117,\n",
              "         'come': 73,\n",
              "         'sirva': 15,\n",
              "         'escusa': 16,\n",
              "         'almoçou': 34,\n",
              "         'anteriores': 79,\n",
              "         'bastando': 8,\n",
              "         'abono': 4,\n",
              "         'alegre': 481,\n",
              "         'certo': 1242,\n",
              "         'tempestade': 70,\n",
              "         'serenara': 1,\n",
              "         'ressaca': 43,\n",
              "         'fort': 2,\n",
              "         'diminuiria': 7,\n",
              "         'evitou': 7,\n",
              "         'falarlhe': 111,\n",
              "         'recordarse': 3,\n",
              "         'dá': 517,\n",
              "         'hás': 169,\n",
              "         'rir': 320,\n",
              "         'teus': 329,\n",
              "         'planos': 70,\n",
              "         'ontem': 540,\n",
              "         'agradece': 10,\n",
              "         'destino': 379,\n",
              "         'haveres': 12,\n",
              "         'escapado': 15,\n",
              "         'depressa': 590,\n",
              "         'conselho': 258,\n",
              "         'dize': 41,\n",
              "         'escrita': 107,\n",
              "         'papel': 775,\n",
              "         'velino': 1,\n",
              "         'corte': 361,\n",
              "         'dourado': 12,\n",
              "         'cheiroso': 3,\n",
              "         'catita': 2,\n",
              "         'parabéns': 47,\n",
              "         'lê': 107,\n",
              "         'pêsames': 33,\n",
              "         'acabou': 635,\n",
              "         'ler': 578,\n",
              "         'chegaste': 8,\n",
              "         'põe': 81,\n",
              "         'epístola': 30,\n",
              "         'fundo': 429,\n",
              "         'gaveta': 45,\n",
              "         'lembres': 5,\n",
              "         'postscriptum': 9,\n",
              "         'aplaudiu': 21,\n",
              "         'metáfora': 42,\n",
              "         'sorriso': 361,\n",
              "         'agouro': 19,\n",
              "         'formosa': 172,\n",
              "         'seguir': 88,\n",
              "         'sentiu': 331,\n",
              "         'abalado': 16,\n",
              "         'ferida': 50,\n",
              "         'cicatrizara': 1,\n",
              "         'segunda': 422,\n",
              "         'pôde': 674,\n",
              "         'encarála': 12,\n",
              "         'perturbação': 8,\n",
              "         'romântico': 10,\n",
              "         'pelo': 1770,\n",
              "         'pusesse': 26,\n",
              "         'caminho': 547,\n",
              "         'lavado': 12,\n",
              "         'bebêlas': 1,\n",
              "         'pedia': 222,\n",
              "         'hei': 261,\n",
              "         'enxutos': 3,\n",
              "         'distraindose': 1,\n",
              "         'tédios': 17,\n",
              "         'pilhéria': 11,\n",
              "         'dantes': 90,\n",
              "         'ii': 249,\n",
              "         'roupão': 26,\n",
              "         'mês': 296,\n",
              "         'chegar': 325,\n",
              "         'achavase': 74,\n",
              "         'definitivamente': 66,\n",
              "         'morta': 134,\n",
              "         'enterrada': 14,\n",
              "         'cantando': 53,\n",
              "         'responso': 9,\n",
              "         'vozes': 180,\n",
              "         'alternadas': 2,\n",
              "         'moças': 261,\n",
              "         'capital': 106,\n",
              "         'elas': 569,\n",
              "         'claro': 311,\n",
              "         'tomou': 215,\n",
              "         'grau': 65,\n",
              "         'bacharel': 139,\n",
              "         'nenhuma': 906,\n",
              "         'restava': 66,\n",
              "         'demais': 612,\n",
              "         'bela': 546,\n",
              "         'deixara': 106,\n",
              "         'morar': 52,\n",
              "         'madrinha': 209,\n",
              "         'vira': 202,\n",
              "         'voltava': 145,\n",
              "         'graduado': 6,\n",
              "         'ciências': 15,\n",
              "         'jurídicas': 7,\n",
              "         'sociais': 56,\n",
              "         'fica': 434,\n",
              "         'dito': 372,\n",
              "         'desejoso': 15,\n",
              "         'devassar': 1,\n",
              "         'futuro': 598,\n",
              "         'reler': 32,\n",
              "         'divertiase': 5,\n",
              "         'divertiu': 4,\n",
              "         'transpuseram': 1,\n",
              "         'linha': 152,\n",
              "         'cinqüenta': 218,\n",
              "         'eterno': 211,\n",
              "         'reparo': 52,\n",
              "         'j': 59,\n",
              "         'á': 159,\n",
              "         'dão': 146,\n",
              "         'seus': 2347,\n",
              "         'primeiros': 472,\n",
              "         'varões': 34,\n",
              "         'maduros': 4,\n",
              "         'mocidade': 282,\n",
              "         'folga': 9,\n",
              "         'deles': 671,\n",
              "         'vê': 376,\n",
              "         'idade': 548,\n",
              "         'recreios': 24,\n",
              "         'juventude': 104,\n",
              "         'decerto': 212,\n",
              "         'igualmente': 202,\n",
              "         'nobres': 22,\n",
              "         'frívolos': 15,\n",
              "         'culpa': 213,\n",
              "         'merecimento': 47,\n",
              "         'cai': 77,\n",
              "         'sorte': 294,\n",
              "         'apesar': 612,\n",
              "         'estragos': 4,\n",
              "         'cólera': 158,\n",
              "         'bailavase': 1,\n",
              "         'cantavase': 2,\n",
              "         'iase5': 1,\n",
              "         'teatro': 513,\n",
              "         'cassino': 34,\n",
              "         'abria': 89,\n",
              "         'salões': 19,\n",
              "         'clube': 26,\n",
              "         'congresso': 30,\n",
              "         'fluminenses': 7,\n",
              "         'homéricos': 1,\n",
              "         'lírico': 82,\n",
              "         'quadra': 35,\n",
              "         'memorável': 11,\n",
              "         'daquelas': 136,\n",
              "         'lutas': 58,\n",
              "         'rivalidades': 3,\n",
              "         'renovadas': 1,\n",
              "         'semestre': 8,\n",
              "         ...})"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EyGVDL9KzJ_I"
      },
      "source": [
        "## Criando um vocabulário"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FiP7OCo9zJ_I"
      },
      "outputs": [],
      "source": [
        "vocab_size = 20000\n",
        "most_frequent_words = [word for word, count in word_counts.most_common(vocab_size)]\n",
        "# o vocabulário irá começar a partir do 3 pois irei reservar o valores 0, 1 e 2 para os tokens especiais <unk>, <sos> e <eos> respectivamente\n",
        "all_tokens = [\"<UNK>\", \"<SOS>\", \"<EOS>\", \"<PAD>\"] + most_frequent_words\n",
        "vocab = {word: i for i, word in enumerate(all_tokens)}"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "DiZYTKA06ID9",
        "outputId": "b6116e99-1baa-48ae-f583-20e93e9ed2c8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'<UNK>': 0,\n",
              " '<SOS>': 1,\n",
              " '<EOS>': 2,\n",
              " '<PAD>': 3,\n",
              " 'a': 4,\n",
              " 'que': 5,\n",
              " 'de': 6,\n",
              " 'e': 7,\n",
              " 'o': 8,\n",
              " 'não': 9,\n",
              " 'um': 10,\n",
              " 'do': 11,\n",
              " 'da': 12,\n",
              " 'é': 13,\n",
              " 'os': 14,\n",
              " 'com': 15,\n",
              " 'se': 16,\n",
              " 'uma': 17,\n",
              " 'em': 18,\n",
              " 'mas': 19,\n",
              " 'para': 20,\n",
              " 'as': 21,\n",
              " 'era': 22,\n",
              " 'ao': 23,\n",
              " 'por': 24,\n",
              " 'no': 25,\n",
              " 'à': 26,\n",
              " 'eu': 27,\n",
              " 'mais': 28,\n",
              " 'na': 29,\n",
              " 'como': 30,\n",
              " 'lhe': 31,\n",
              " 'ele': 32,\n",
              " 'me': 33,\n",
              " 'ou': 34,\n",
              " 'foi': 35,\n",
              " 'dos': 36,\n",
              " 'ela': 37,\n",
              " 'nem': 38,\n",
              " 'disse': 39,\n",
              " 'quando': 40,\n",
              " 'das': 41,\n",
              " 'sem': 42,\n",
              " 'já': 43,\n",
              " 'casa': 44,\n",
              " 'meu': 45,\n",
              " 'depois': 46,\n",
              " 'há': 47,\n",
              " 'minha': 48,\n",
              " 'ser': 49,\n",
              " 'tudo': 50,\n",
              " 'só': 51,\n",
              " 'tempo': 52,\n",
              " 'tinha': 53,\n",
              " 'olhos': 54,\n",
              " 'nada': 55,\n",
              " 'ainda': 56,\n",
              " 'muito': 57,\n",
              " 'd': 58,\n",
              " 'também': 59,\n",
              " 'outra': 60,\n",
              " 'dia': 61,\n",
              " 'outro': 62,\n",
              " 'mesmo': 63,\n",
              " 'tão': 64,\n",
              " 'esta': 65,\n",
              " 'seu': 66,\n",
              " 'sua': 67,\n",
              " 'estava': 68,\n",
              " 'vez': 69,\n",
              " 'até': 70,\n",
              " 'porque': 71,\n",
              " 'nos': 72,\n",
              " 'este': 73,\n",
              " 'assim': 74,\n",
              " 'pouco': 75,\n",
              " 'agora': 76,\n",
              " 'dias': 77,\n",
              " 'às': 78,\n",
              " 'vida': 79,\n",
              " 'aos': 80,\n",
              " 'bem': 81,\n",
              " 'então': 82,\n",
              " 'alguma': 83,\n",
              " 'homem': 84,\n",
              " 'duas': 85,\n",
              " 'quem': 86,\n",
              " 'menos': 87,\n",
              " 'amor': 88,\n",
              " 'lá': 89,\n",
              " 'todos': 90,\n",
              " 'isso': 91,\n",
              " 'podia': 92,\n",
              " 'coisa': 93,\n",
              " 'dizer': 94,\n",
              " 'ia': 95,\n",
              " 'coração': 96,\n",
              " 'moça': 97,\n",
              " 'mim': 98,\n",
              " 'eram': 99,\n",
              " 'anos': 100,\n",
              " 'entre': 101,\n",
              " 'seus': 102,\n",
              " 'pode': 103,\n",
              " 'alguns': 104,\n",
              " 'pai': 105,\n",
              " 'está': 106,\n",
              " 'pela': 107,\n",
              " 'isto': 108,\n",
              " 'dois': 109,\n",
              " 'logo': 110,\n",
              " 'ver': 111,\n",
              " 'capítulo': 112,\n",
              " 'verdade': 113,\n",
              " 'mãe': 114,\n",
              " 'são': 115,\n",
              " 'grande': 116,\n",
              " 'antes': 117,\n",
              " 'tem': 118,\n",
              " 'noite': 119,\n",
              " 'carta': 120,\n",
              " 'sei': 121,\n",
              " 'fosse': 122,\n",
              " 'mesma': 123,\n",
              " 'melhor': 124,\n",
              " 'si': 125,\n",
              " 'the': 126,\n",
              " 'mal': 127,\n",
              " 'ir': 128,\n",
              " 'ter': 129,\n",
              " 'aqui': 130,\n",
              " 'onde': 131,\n",
              " 'mão': 132,\n",
              " 'outros': 133,\n",
              " 'palavras': 134,\n",
              " 'esse': 135,\n",
              " 'algum': 136,\n",
              " 'pelo': 137,\n",
              " 'talvez': 138,\n",
              " 'nunca': 139,\n",
              " 'havia': 140,\n",
              " 'três': 141,\n",
              " 'idéia': 142,\n",
              " 'marido': 143,\n",
              " 'todas': 144,\n",
              " 'nas': 145,\n",
              " 'dele': 146,\n",
              " 'tal': 147,\n",
              " 'mulher': 148,\n",
              " 'alma': 149,\n",
              " 'fim': 150,\n",
              " 'rua': 151,\n",
              " 'fazer': 152,\n",
              " 'sim': 153,\n",
              " 'fora': 154,\n",
              " 'sempre': 155,\n",
              " 'amigo': 156,\n",
              " 'suas': 157,\n",
              " 'vezes': 158,\n",
              " 'todo': 159,\n",
              " 'senhor': 160,\n",
              " 'fez': 161,\n",
              " 'toda': 162,\n",
              " 'br': 163,\n",
              " 'velho': 164,\n",
              " 'primeiro': 165,\n",
              " 'caso': 166,\n",
              " 'primeira': 167,\n",
              " 'essa': 168,\n",
              " 'ali': 169,\n",
              " 'algumas': 170,\n",
              " 's': 171,\n",
              " 'eles': 172,\n",
              " 'dela': 173,\n",
              " 'parte': 174,\n",
              " 'outras': 175,\n",
              " 'você': 176,\n",
              " 'cabeça': 177,\n",
              " 'mãos': 178,\n",
              " 'filha': 179,\n",
              " 'meio': 180,\n",
              " 'espírito': 181,\n",
              " 'porta': 182,\n",
              " 'pois': 183,\n",
              " 'vai': 184,\n",
              " 'cousa': 185,\n",
              " 'dizia': 186,\n",
              " 'horas': 187,\n",
              " 'tanto': 188,\n",
              " 'senhora': 189,\n",
              " 'jorge': 190,\n",
              " 'tu': 191,\n",
              " 'sobre': 192,\n",
              " 'hoje': 193,\n",
              " 'deu': 194,\n",
              " 'creio': 195,\n",
              " 'nead': 196,\n",
              " 'te': 197,\n",
              " 'mundo': 198,\n",
              " 'rapaz': 199,\n",
              " 'helena': 200,\n",
              " 'apenas': 201,\n",
              " 'dar': 202,\n",
              " 'parece': 203,\n",
              " 'tarde': 204,\n",
              " 'razão': 205,\n",
              " 'padre': 206,\n",
              " 'filho': 207,\n",
              " 'unama': 208,\n",
              " 'luís': 209,\n",
              " 'ninguém': 210,\n",
              " 'certo': 211,\n",
              " 'palavra': 212,\n",
              " 'dous': 213,\n",
              " 'respondeu': 214,\n",
              " 'pé': 215,\n",
              " 'seria': 216,\n",
              " 'gente': 217,\n",
              " 'boa': 218,\n",
              " 'meus': 219,\n",
              " 'perguntou': 220,\n",
              " 'nome': 221,\n",
              " 'viúva': 222,\n",
              " 'bom': 223,\n",
              " 'ficou': 224,\n",
              " 'deus': 225,\n",
              " 'coisas': 226,\n",
              " 'modo': 227,\n",
              " 'sala': 228,\n",
              " 'quer': 229,\n",
              " 'saber': 230,\n",
              " 'tenho': 231,\n",
              " 'morte': 232,\n",
              " 'ambos': 233,\n",
              " 'céu': 234,\n",
              " 'ora': 235,\n",
              " 'desde': 236,\n",
              " 'casamento': 237,\n",
              " 'olhar': 238,\n",
              " 'hora': 239,\n",
              " 'qual': 240,\n",
              " 'of': 241,\n",
              " 'comigo': 242,\n",
              " 'estácio': 243,\n",
              " 'gesto': 244,\n",
              " 'pedro': 245,\n",
              " 'pessoa': 246,\n",
              " 'preciso': 247,\n",
              " 'capitu': 248,\n",
              " 'queria': 249,\n",
              " 'família': 250,\n",
              " 'nossa': 251,\n",
              " 'vou': 252,\n",
              " 'viu': 253,\n",
              " 'veio': 254,\n",
              " 'falar': 255,\n",
              " 'ar': 256,\n",
              " 'enfim': 257,\n",
              " 'quis': 258,\n",
              " 'fui': 259,\n",
              " 'estou': 260,\n",
              " 'aquela': 261,\n",
              " 'sou': 262,\n",
              " 'porém': 263,\n",
              " 'josé': 264,\n",
              " 'livro': 265,\n",
              " 'pessoas': 266,\n",
              " 'minutos': 267,\n",
              " 'sr': 268,\n",
              " 'amigos': 269,\n",
              " 'entrou': 270,\n",
              " 'sabe': 271,\n",
              " 'terra': 272,\n",
              " 'lado': 273,\n",
              " 'voz': 274,\n",
              " 'nenhuma': 275,\n",
              " 'ah': 276,\n",
              " 'paulo': 277,\n",
              " 'simples': 278,\n",
              " 'cinco': 279,\n",
              " 'vi': 280,\n",
              " 'cada': 281,\n",
              " 'parecia': 282,\n",
              " 'aquele': 283,\n",
              " 'teve': 284,\n",
              " 'saiu': 285,\n",
              " 'foram': 286,\n",
              " 'estas': 287,\n",
              " 'nosso': 288,\n",
              " 'longe': 289,\n",
              " 'rosto': 290,\n",
              " 'homens': 291,\n",
              " 'seja': 292,\n",
              " 'ponto': 293,\n",
              " 'uns': 294,\n",
              " 'jantar': 295,\n",
              " 'dentro': 296,\n",
              " 'mendonça': 297,\n",
              " 'nenhum': 298,\n",
              " 'meses': 299,\n",
              " 'nova': 300,\n",
              " 'deste': 301,\n",
              " 'seguinte': 302,\n",
              " 'nós': 303,\n",
              " 'maior': 304,\n",
              " 'triste': 305,\n",
              " 'to': 306,\n",
              " 'poeta': 307,\n",
              " 'quanto': 308,\n",
              " 'adelaide': 309,\n",
              " 'gosto': 310,\n",
              " 'quase': 311,\n",
              " 'sair': 312,\n",
              " 'digo': 313,\n",
              " 'devia': 314,\n",
              " 'ocasião': 315,\n",
              " 'quatro': 316,\n",
              " 'manhã': 317,\n",
              " 'chegou': 318,\n",
              " 'posso': 319,\n",
              " 'próprio': 320,\n",
              " 'vinha': 321,\n",
              " 'muita': 322,\n",
              " 'teu': 323,\n",
              " 'lugar': 324,\n",
              " 'tua': 325,\n",
              " 'cá': 326,\n",
              " 'quero': 327,\n",
              " 'papel': 328,\n",
              " 'tio': 329,\n",
              " 'silêncio': 330,\n",
              " 'fazia': 331,\n",
              " 'durante': 332,\n",
              " 'resposta': 333,\n",
              " 'i': 334,\n",
              " 'v': 335,\n",
              " 'causa': 336,\n",
              " 'desta': 337,\n",
              " 'and': 338,\n",
              " 'naturalmente': 339,\n",
              " 'rio': 340,\n",
              " 'daí': 341,\n",
              " 'iaiá': 342,\n",
              " 'força': 343,\n",
              " 'sabia': 344,\n",
              " 'vinte': 345,\n",
              " 'lágrimas': 346,\n",
              " 'estar': 347,\n",
              " 'achou': 348,\n",
              " 'boca': 349,\n",
              " 'aí': 350,\n",
              " 'aires': 351,\n",
              " 'sol': 352,\n",
              " 'resto': 353,\n",
              " 'felicidade': 354,\n",
              " 'qualquer': 355,\n",
              " 'natural': 356,\n",
              " 'faz': 357,\n",
              " 'continuou': 358,\n",
              " 'estela': 359,\n",
              " 'além': 360,\n",
              " 'moço': 361,\n",
              " 'ano': 362,\n",
              " 'entretanto': 363,\n",
              " 'pôde': 364,\n",
              " 'própria': 365,\n",
              " 'deles': 366,\n",
              " 'dez': 367,\n",
              " 'enquanto': 368,\n",
              " 'entrar': 369,\n",
              " 'diante': 370,\n",
              " 'novo': 371,\n",
              " 'tinham': 372,\n",
              " 'tito': 373,\n",
              " 'leitor': 374,\n",
              " 'flor': 375,\n",
              " 'notícia': 376,\n",
              " 'natureza': 377,\n",
              " 'flora': 378,\n",
              " 'quê': 379,\n",
              " 'médico': 380,\n",
              " 'janela': 381,\n",
              " 'vontade': 382,\n",
              " 'fidélia': 383,\n",
              " 'tristão': 384,\n",
              " 'pobre': 385,\n",
              " 'dizendo': 386,\n",
              " 'política': 387,\n",
              " 'velha': 388,\n",
              " 'acabou': 389,\n",
              " 'situação': 390,\n",
              " 'neste': 391,\n",
              " 'efeito': 392,\n",
              " 'história': 393,\n",
              " 'muitas': 394,\n",
              " 'cartas': 395,\n",
              " 'aguiar': 396,\n",
              " 'dava': 397,\n",
              " 'doutor': 398,\n",
              " 'dor': 399,\n",
              " 'passado': 400,\n",
              " 'ouvir': 401,\n",
              " 'vão': 402,\n",
              " 'demais': 403,\n",
              " 'apesar': 404,\n",
              " 'grandes': 405,\n",
              " 'tais': 406,\n",
              " 'graça': 407,\n",
              " 'ficar': 408,\n",
              " 'esperança': 409,\n",
              " 'amiga': 410,\n",
              " 'princípio': 411,\n",
              " 'feliz': 412,\n",
              " 'emília': 413,\n",
              " 'vamos': 414,\n",
              " 'motivo': 415,\n",
              " 'futuro': 416,\n",
              " 'idéias': 417,\n",
              " 'oh': 418,\n",
              " 'vir': 419,\n",
              " 'mesa': 420,\n",
              " 'cidade': 421,\n",
              " 'virgília': 422,\n",
              " 'certa': 423,\n",
              " 'sentimento': 424,\n",
              " 'depressa': 425,\n",
              " 'soares': 426,\n",
              " 'dr': 427,\n",
              " 'vem': 428,\n",
              " 'possível': 429,\n",
              " 'consigo': 430,\n",
              " 'costume': 431,\n",
              " 'amanhã': 432,\n",
              " 'segundo': 433,\n",
              " 'ouviu': 434,\n",
              " 'contrário': 435,\n",
              " 'passo': 436,\n",
              " 'ler': 437,\n",
              " 'luz': 438,\n",
              " 'será': 439,\n",
              " 'casar': 440,\n",
              " 'disselhe': 441,\n",
              " 'nesse': 442,\n",
              " 'última': 443,\n",
              " 'houve': 444,\n",
              " 'elas': 445,\n",
              " 'passou': 446,\n",
              " 'cousas': 447,\n",
              " 'lhes': 448,\n",
              " 'contra': 449,\n",
              " 'in': 450,\n",
              " 'assis': 451,\n",
              " 'quarto': 452,\n",
              " 'estavam': 453,\n",
              " 'opinião': 454,\n",
              " 'natividade': 455,\n",
              " 'desejo': 456,\n",
              " 'respeito': 457,\n",
              " 'carlota': 458,\n",
              " 'idade': 459,\n",
              " 'caminho': 460,\n",
              " 'viagem': 461,\n",
              " 'bela': 462,\n",
              " 'mar': 463,\n",
              " 'fortuna': 464,\n",
              " 'carmo': 465,\n",
              " 'ontem': 466,\n",
              " 'chão': 467,\n",
              " 'janeiro': 468,\n",
              " 'pena': 469,\n",
              " 'trazia': 470,\n",
              " 'deve': 471,\n",
              " 'diz': 472,\n",
              " 'glória': 473,\n",
              " 'vá': 474,\n",
              " 'obra': 475,\n",
              " 'vasconcelos': 476,\n",
              " 'paixão': 477,\n",
              " 'naquele': 478,\n",
              " 'câmara': 479,\n",
              " 'deixou': 480,\n",
              " 'via': 481,\n",
              " 'dá': 482,\n",
              " 'joão': 483,\n",
              " 'tivesse': 484,\n",
              " 'iam': 485,\n",
              " 'último': 486,\n",
              " 'teatro': 487,\n",
              " 'voltou': 488,\n",
              " 'filhos': 489,\n",
              " 'irmão': 490,\n",
              " 'crer': 491,\n",
              " 'm': 492,\n",
              " 'expressão': 493,\n",
              " 'começou': 494,\n",
              " 'disso': 495,\n",
              " 'senão': 496,\n",
              " 'rita': 497,\n",
              " 'daquele': 498,\n",
              " 'conversa': 499,\n",
              " 'minhas': 500,\n",
              " 'estado': 501,\n",
              " 'trabalho': 502,\n",
              " 'realmente': 503,\n",
              " 'naquela': 504,\n",
              " 'daquela': 505,\n",
              " 'muitos': 506,\n",
              " 'alegria': 507,\n",
              " 'embora': 508,\n",
              " 'carro': 509,\n",
              " 'braços': 510,\n",
              " 'passar': 511,\n",
              " 'meia': 512,\n",
              " 'alegre': 513,\n",
              " 'adeus': 514,\n",
              " 'ouvi': 515,\n",
              " 'machado': 516,\n",
              " 'medo': 517,\n",
              " 'for': 518,\n",
              " 'haver': 519,\n",
              " 'disseme': 520,\n",
              " 'falou': 521,\n",
              " 'santos': 522,\n",
              " 'primeiros': 523,\n",
              " 'pés': 524,\n",
              " 'vista': 525,\n",
              " 'cara': 526,\n",
              " 'figura': 527,\n",
              " 'momento': 528,\n",
              " 'instante': 529,\n",
              " 'pareceu': 530,\n",
              " 'memória': 531,\n",
              " 'daqui': 532,\n",
              " 'feito': 533,\n",
              " 'morrer': 534,\n",
              " 'deixar': 535,\n",
              " 'ernesto': 536,\n",
              " 'assunto': 537,\n",
              " 'tive': 538,\n",
              " 'num': 539,\n",
              " 'estêvão': 540,\n",
              " 'alves': 541,\n",
              " 'dona': 542,\n",
              " 'falava': 543,\n",
              " 'flores': 544,\n",
              " 'posto': 545,\n",
              " 'necessidade': 546,\n",
              " 'garcia': 547,\n",
              " 'poucos': 548,\n",
              " 'estes': 549,\n",
              " 'versos': 550,\n",
              " 'pelos': 551,\n",
              " 'impossível': 552,\n",
              " 'consciência': 553,\n",
              " 'viver': 554,\n",
              " 'igreja': 555,\n",
              " 'cor': 556,\n",
              " 'vale': 557,\n",
              " 'cedo': 558,\n",
              " 'fica': 559,\n",
              " 'prazer': 560,\n",
              " 'arte': 561,\n",
              " 'autor': 562,\n",
              " 'numa': 563,\n",
              " 'desse': 564,\n",
              " 'olhou': 565,\n",
              " 'corpo': 566,\n",
              " 'umas': 567,\n",
              " 'irmã': 568,\n",
              " 'fundo': 569,\n",
              " 'prima': 570,\n",
              " 'criança': 571,\n",
              " 'seis': 572,\n",
              " 'escrever': 573,\n",
              " 'segunda': 574,\n",
              " 'tendo': 575,\n",
              " 'essas': 576,\n",
              " 'instantes': 577,\n",
              " 'aliás': 578,\n",
              " 'sentimentos': 579,\n",
              " 'clara': 580,\n",
              " 'achava': 581,\n",
              " 'exemplo': 582,\n",
              " 'delas': 583,\n",
              " 'visto': 584,\n",
              " 'perto': 585,\n",
              " 'justamente': 586,\n",
              " 'morreu': 587,\n",
              " 'fiz': 588,\n",
              " 'ato': 589,\n",
              " 'tia': 590,\n",
              " 'interesse': 591,\n",
              " 'alguém': 592,\n",
              " 'sangue': 593,\n",
              " 'mamãe': 594,\n",
              " 'atenção': 595,\n",
              " 'nele': 596,\n",
              " 'dinheiro': 597,\n",
              " 'governo': 598,\n",
              " 'beleza': 599,\n",
              " 'diabo': 600,\n",
              " 'única': 601,\n",
              " 'sorriu': 602,\n",
              " 'concluiu': 603,\n",
              " 'conselheiro': 604,\n",
              " 'cama': 605,\n",
              " 'maneira': 606,\n",
              " 'pudesse': 607,\n",
              " 'ti': 608,\n",
              " 'mau': 609,\n",
              " 'cena': 610,\n",
              " 'direito': 611,\n",
              " 'soube': 612,\n",
              " 'livros': 613,\n",
              " 'único': 614,\n",
              " 'dali': 615,\n",
              " 'oito': 616,\n",
              " 'cadeira': 617,\n",
              " 'cima': 618,\n",
              " 'maria': 619,\n",
              " 'véspera': 620,\n",
              " 'grave': 621,\n",
              " 'he': 622,\n",
              " 'tempos': 623,\n",
              " 'capaz': 624,\n",
              " 'santa': 625,\n",
              " 'azevedo': 626,\n",
              " 'missa': 627,\n",
              " 'igual': 628,\n",
              " 'espécie': 629,\n",
              " 'impressão': 630,\n",
              " 'paz': 631,\n",
              " 'doente': 632,\n",
              " 'alto': 633,\n",
              " 'achei': 634,\n",
              " 'contos': 635,\n",
              " 'particular': 636,\n",
              " 'ordem': 637,\n",
              " 'volta': 638,\n",
              " 'cair': 639,\n",
              " 'passos': 640,\n",
              " 'menina': 641,\n",
              " 'sentia': 642,\n",
              " 'doce': 643,\n",
              " 'afeição': 644,\n",
              " 'diogo': 645,\n",
              " 'antônia': 646,\n",
              " 'mulheres': 647,\n",
              " 'destino': 648,\n",
              " 'presente': 649,\n",
              " 'cabelos': 650,\n",
              " 'teria': 651,\n",
              " 'vê': 652,\n",
              " 'pensamento': 653,\n",
              " 'saudades': 654,\n",
              " 'somente': 655,\n",
              " 'dito': 656,\n",
              " 'pergunta': 657,\n",
              " 'discurso': 658,\n",
              " 'visita': 659,\n",
              " 'estão': 660,\n",
              " 'afinal': 661,\n",
              " 'chapéu': 662,\n",
              " 'contar': 663,\n",
              " 'trinta': 664,\n",
              " 'mil': 665,\n",
              " 'achar': 666,\n",
              " 'longo': 667,\n",
              " 'segredo': 668,\n",
              " 'aquilo': 669,\n",
              " 'pelas': 670,\n",
              " 'fala': 671,\n",
              " 'cabo': 672,\n",
              " 'longa': 673,\n",
              " 'corte': 674,\n",
              " 'sorriso': 675,\n",
              " 'europa': 676,\n",
              " 'that': 677,\n",
              " 'diga': 678,\n",
              " 'major': 679,\n",
              " 'nessa': 680,\n",
              " 'amores': 681,\n",
              " 'esposa': 682,\n",
              " 'simão': 683,\n",
              " 'ficava': 684,\n",
              " 'viva': 685,\n",
              " 'acho': 686,\n",
              " 'almoço': 687,\n",
              " 'intenção': 688,\n",
              " 'pude': 689,\n",
              " 'moral': 690,\n",
              " 'perder': 691,\n",
              " 'lembrança': 692,\n",
              " 'sono': 693,\n",
              " 'poderia': 694,\n",
              " 'basta': 695,\n",
              " 'atrás': 696,\n",
              " 'esperanças': 697,\n",
              " 'forma': 698,\n",
              " 'bilhete': 699,\n",
              " 'voltar': 700,\n",
              " 'reflexão': 701,\n",
              " 'deveras': 702,\n",
              " 'difícil': 703,\n",
              " 'ação': 704,\n",
              " 'nela': 705,\n",
              " 'falta': 706,\n",
              " 'alienista': 707,\n",
              " 'pior': 708,\n",
              " 'realidade': 709,\n",
              " 'jardim': 710,\n",
              " 'tristeza': 711,\n",
              " 'recebeu': 712,\n",
              " 'contava': 713,\n",
              " 'ambas': 714,\n",
              " 'dúvida': 715,\n",
              " 'língua': 716,\n",
              " 'his': 717,\n",
              " 'sendo': 718,\n",
              " 'úrsula': 719,\n",
              " 'nariz': 720,\n",
              " 'caro': 721,\n",
              " 'verde': 722,\n",
              " 'bonita': 723,\n",
              " 'criado': 724,\n",
              " 'faria': 725,\n",
              " 'sociedade': 726,\n",
              " 'explicação': 727,\n",
              " 'sentiu': 728,\n",
              " 'és': 729,\n",
              " 'acabar': 730,\n",
              " 'pensou': 731,\n",
              " 'conversação': 732,\n",
              " 'forte': 733,\n",
              " 'vieram': 734,\n",
              " 'escobar': 735,\n",
              " 'braço': 736,\n",
              " 'sombra': 737,\n",
              " 'teus': 738,\n",
              " 'acaso': 739,\n",
              " 'dado': 740,\n",
              " 'nota': 741,\n",
              " 'batista': 742,\n",
              " 'diferença': 743,\n",
              " 'poder': 744,\n",
              " 'esperar': 745,\n",
              " 'exclamou': 746,\n",
              " 'chegar': 747,\n",
              " 'fiquei': 748,\n",
              " 'sorrindo': 749,\n",
              " 'augusta': 750,\n",
              " 'verdadeira': 751,\n",
              " 'curiosidade': 752,\n",
              " 'rir': 753,\n",
              " 'podiam': 754,\n",
              " 'esses': 755,\n",
              " 'pais': 756,\n",
              " 'gomes': 757,\n",
              " 'provavelmente': 758,\n",
              " 'dei': 759,\n",
              " 'público': 760,\n",
              " 'dizem': 761,\n",
              " 'podem': 762,\n",
              " 'negócio': 763,\n",
              " 'propósito': 764,\n",
              " 'margarida': 765,\n",
              " 'tens': 766,\n",
              " 'claro': 767,\n",
              " 'esperava': 768,\n",
              " 'amar': 769,\n",
              " 'pensava': 770,\n",
              " 'vejo': 771,\n",
              " 'prova': 772,\n",
              " 'frase': 773,\n",
              " 'uso': 774,\n",
              " 'conta': 775,\n",
              " 'velhos': 776,\n",
              " 'pedir': 777,\n",
              " 'dedos': 778,\n",
              " 'apareceu': 779,\n",
              " 'barão': 780,\n",
              " 'acabava': 781,\n",
              " 'tornou': 782,\n",
              " 'espera': 783,\n",
              " 'tom': 784,\n",
              " 'semanas': 785,\n",
              " 'pura': 786,\n",
              " 'número': 787,\n",
              " 'was': 788,\n",
              " 'amizade': 789,\n",
              " 'poesia': 790,\n",
              " 'visitas': 791,\n",
              " 'pequena': 792,\n",
              " 'perdão': 793,\n",
              " 'is': 794,\n",
              " 'confiança': 795,\n",
              " 'ama': 796,\n",
              " 'imaginação': 797,\n",
              " 'azul': 798,\n",
              " 'nisto': 799,\n",
              " 'riso': 800,\n",
              " 'lábios': 801,\n",
              " 'leu': 802,\n",
              " 'têm': 803,\n",
              " 'iria': 804,\n",
              " 'baixo': 805,\n",
              " 'nesta': 806,\n",
              " 'andar': 807,\n",
              " 'mês': 808,\n",
              " 'graças': 809,\n",
              " 'mirto': 810,\n",
              " 'sorte': 811,\n",
              " 'bons': 812,\n",
              " 'lei': 813,\n",
              " 'houvesse': 814,\n",
              " 'fossem': 815,\n",
              " 'abriu': 816,\n",
              " 'olhe': 817,\n",
              " 'liberdade': 818,\n",
              " 'escola': 819,\n",
              " 'gostava': 820,\n",
              " 'sabes': 821,\n",
              " 'favor': 822,\n",
              " 'página': 823,\n",
              " 'ciência': 824,\n",
              " 'c': 825,\n",
              " 'inteiramente': 826,\n",
              " 'acima': 827,\n",
              " 'certeza': 828,\n",
              " 'criatura': 829,\n",
              " 'movimento': 830,\n",
              " 'ministro': 831,\n",
              " 'rindo': 832,\n",
              " 'finalmente': 833,\n",
              " 'questão': 834,\n",
              " 'guiomar': 835,\n",
              " 'remédio': 836,\n",
              " 'dormir': 837,\n",
              " 'parecer': 838,\n",
              " 'petrópolis': 839,\n",
              " 'retrato': 840,\n",
              " 'seminário': 841,\n",
              " 'chácara': 842,\n",
              " 'boas': 843,\n",
              " 'eis': 844,\n",
              " 'nossas': 845,\n",
              " 'pareciam': 846,\n",
              " 'olhava': 847,\n",
              " 'sido': 848,\n",
              " 'dama': 849,\n",
              " 'nomes': 850,\n",
              " 'esquerda': 851,\n",
              " 'mocidade': 852,\n",
              " 'folhas': 853,\n",
              " 'passeio': 854,\n",
              " 'notícias': 855,\n",
              " 'vendo': 856,\n",
              " 'asas': 857,\n",
              " 'contudo': 858,\n",
              " 'ternura': 859,\n",
              " 'pedido': 860,\n",
              " 'sonho': 861,\n",
              " 'ausência': 862,\n",
              " 'escreveu': 863,\n",
              " 'amava': 864,\n",
              " 'ofício': 865,\n",
              " 'baronesa': 866,\n",
              " 'rapazes': 867,\n",
              " 'resolução': 868,\n",
              " 'cuja': 869,\n",
              " 'noites': 870,\n",
              " 'primeiras': 871,\n",
              " 'interrompeu': 872,\n",
              " 'senhoras': 873,\n",
              " 'largo': 874,\n",
              " 'nossos': 875,\n",
              " 'andava': 876,\n",
              " 'peito': 877,\n",
              " 'pequeno': 878,\n",
              " 'estilo': 879,\n",
              " 'obrigado': 880,\n",
              " 'música': 881,\n",
              " 'praia': 882,\n",
              " 'pediu': 883,\n",
              " 'vim': 884,\n",
              " 'gabinete': 885,\n",
              " 'povo': 886,\n",
              " 'ouvia': 887,\n",
              " 'canto': 888,\n",
              " 'dessas': 889,\n",
              " 'bonito': 890,\n",
              " 'promessa': 891,\n",
              " 'sentido': 892,\n",
              " 'levava': 893,\n",
              " 'deixa': 894,\n",
              " 'eugênia': 895,\n",
              " 'importa': 896,\n",
              " 'anda': 897,\n",
              " 'quais': 898,\n",
              " 'sobrinho': 899,\n",
              " 'desembargador': 900,\n",
              " 'semelhante': 901,\n",
              " 'dera': 902,\n",
              " 'conhecia': 903,\n",
              " 'qualidades': 904,\n",
              " 'baile': 905,\n",
              " 'antigo': 906,\n",
              " 'principalmente': 907,\n",
              " 'cavalo': 908,\n",
              " 'melchior': 909,\n",
              " 'juntos': 910,\n",
              " 'menor': 911,\n",
              " 'circunstâncias': 912,\n",
              " 'olhando': 913,\n",
              " 'ouro': 914,\n",
              " 'devo': 915,\n",
              " 'presença': 916,\n",
              " 'lembra': 917,\n",
              " 'almas': 918,\n",
              " 'excelente': 919,\n",
              " 'levar': 920,\n",
              " 'hei': 921,\n",
              " 'moças': 922,\n",
              " 'quarenta': 923,\n",
              " 'comum': 924,\n",
              " 'mesmos': 925,\n",
              " 'sete': 926,\n",
              " 'mestre': 927,\n",
              " 'guerra': 928,\n",
              " 'tanta': 929,\n",
              " 'espaço': 930,\n",
              " 'beijo': 931,\n",
              " 'água': 932,\n",
              " 'mana': 933,\n",
              " 'vivia': 934,\n",
              " 'conselho': 935,\n",
              " 'mandou': 936,\n",
              " 'ouvido': 937,\n",
              " 'venha': 938,\n",
              " 'interior': 939,\n",
              " 'poucas': 940,\n",
              " 'antiga': 941,\n",
              " 'frio': 942,\n",
              " 'vestido': 943,\n",
              " 'direita': 944,\n",
              " 'século': 945,\n",
              " 'correu': 946,\n",
              " 'coronel': 947,\n",
              " 'namorado': 948,\n",
              " 'esteve': 949,\n",
              " 'levantouse': 950,\n",
              " 'quisera': 951,\n",
              " 'letras': 952,\n",
              " 'semana': 953,\n",
              " 'capitão': 954,\n",
              " 'breve': 955,\n",
              " 'it': 956,\n",
              " 'sobrinha': 957,\n",
              " 'papai': 958,\n",
              " 'cleon': 959,\n",
              " 'sós': 960,\n",
              " 'ii': 961,\n",
              " 'luta': 962,\n",
              " 'quinze': 963,\n",
              " 'confesso': 964,\n",
              " 'acrescentou': 965,\n",
              " 'relógio': 966,\n",
              " 'ex': 967,\n",
              " 'meneses': 968,\n",
              " 'levou': 969,\n",
              " 'geral': 970,\n",
              " 'festa': 971,\n",
              " 'anterior': 972,\n",
              " 'forças': 973,\n",
              " 'piano': 974,\n",
              " 'tomar': 975,\n",
              " 'ó': 976,\n",
              " 'jornais': 977,\n",
              " 'estivesse': 978,\n",
              " 'acha': 979,\n",
              " 'cheguei': 980,\n",
              " 'original': 981,\n",
              " 'literatura': 982,\n",
              " 'esquecer': 983,\n",
              " 'estás': 984,\n",
              " 'deputado': 985,\n",
              " 'verdadeiro': 986,\n",
              " 'posição': 987,\n",
              " 'acerca': 988,\n",
              " 'sentouse': 989,\n",
              " 'andando': 990,\n",
              " 'dever': 991,\n",
              " 'acudiu': 992,\n",
              " 'novas': 993,\n",
              " 'fazendo': 994,\n",
              " 'sério': 995,\n",
              " 'morto': 996,\n",
              " 'páginas': 997,\n",
              " 'ombros': 998,\n",
              " 'pegou': 999,\n",
              " ...}"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# o tamanho total do vocab será de 10000 das palavras + 3 do tokens especiais\n",
        "len(vocab)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K8FJft348OUh",
        "outputId": "9c14a6b5-db5a-4f0b-8b9b-0f02eb501545"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "20004"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = len(vocab)"
      ],
      "metadata": {
        "id": "heDwbZ4t8hot"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Por que deletar palavras desconhecidas?\n",
        "# estou deletando a frase inteira para garantir que haja contexto que o modelo possa se basear ao fazer a predição da próxima palavra\n",
        "def removeUnknownVocab(texts, vocab):\n",
        "    # Remove frases que contenham palavras fora do vocabulario\n",
        "    validas = []\n",
        "    for sentence in texts:\n",
        "        # só mantém se todas as palavras estão no vocabulário\n",
        "        if all(word in vocab for word in sentence):\n",
        "            validas.append(sentence)\n",
        "    return validas"
      ],
      "metadata": {
        "id": "j_W61pG1VavJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cleaned_train_text = removeUnknownVocab(preprocess_trained_text, vocab)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "8TnT6QfBWDtR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(cleaned_train_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qYxfxawRWSjW",
        "outputId": "76be519a-c570-4857-9177-5be12036aa03",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "17126"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cleaned_val_text = removeUnknownVocab(preprocess_val_text, vocab)"
      ],
      "metadata": {
        "id": "mjp74MW2WquG",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(cleaned_val_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CqfgocfQWwK2",
        "outputId": "a132416f-c5a6-4cda-ef88-61a9a38b041c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3362"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cleaned_test_text = removeUnknownVocab(preprocess_test_text, vocab)"
      ],
      "metadata": {
        "id": "uLbRkkSR2xvh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(cleaned_test_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l5XpkbTs21ct",
        "outputId": "711f3ed9-9e0e-4719-8a99-740fc15082ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1611"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cleaned_train_text[26]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "wzgnWGeuZY85",
        "outputId": "76f04ca1-fe7b-40a6-cfc1-8ba37f807fdc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['há']"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wia_ygbvzJ_J"
      },
      "source": [
        "## Classe do dataset"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenizer(sentence, vocab):\n",
        "    #print(sentence)\n",
        "    if isinstance(sentence, str):\n",
        "        words = [sentence]  # trata como palavra inteira\n",
        "    else:\n",
        "        words = sentence  # já é lista de palavras\n",
        "\n",
        "    return [vocab.get(word, vocab[\"<UNK>\"]) for word in words] # 0 for OOV"
      ],
      "metadata": {
        "id": "Jfr5YTcLbjab"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "idx_to_word = {i: w for w, i in vocab.items()}"
      ],
      "metadata": {
        "id": "0gyo6DyOlou5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#def detokenizer(tokens):\n",
        "    #return [idx_to_word[tok] for tok in tokens] # 0 for OOV"
      ],
      "metadata": {
        "id": "X_IxbPKaluyq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def detokenizer(tokens):\n",
        "    return [\n",
        "        \"<PAD>\" if tok == -100 else idx_to_word[tok]\n",
        "        for tok in tokens\n",
        "    ]"
      ],
      "metadata": {
        "id": "TVjDPx979YRf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cleaned_train_text[3]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fuC0MhZEpQm_",
        "outputId": "b5dbcd87-c109-483c-b458-6f1a8850f6ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['departamento', 'nacional', 'do', 'livro']"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "teste = tokenizer(cleaned_train_text[3], vocab)\n",
        "teste"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0r26QY4MbpFL",
        "outputId": "16e5da68-a454-488e-9b11-ffb4ffab490a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[12221, 1555, 11, 265]"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "det = detokenizer(teste)\n",
        "det"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V1_fRtXLmLFN",
        "outputId": "59d415b0-33c2-470c-cb11-353333118ea9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['departamento', 'nacional', 'do', 'livro']"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cleaned_train_text[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zumoECiWGcjb",
        "outputId": "78e60911-08a4-4be2-8b38-2334f0c17ac0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['1'],\n",
              " ['ministério', 'da', 'cultura'],\n",
              " ['fundação', 'biblioteca', 'nacional'],\n",
              " ['departamento', 'nacional', 'do', 'livro'],\n",
              " ['a', 'mão', 'e', 'a', 'luva'],\n",
              " ['machado', 'de', 'assis'],\n",
              " ['i'],\n",
              " ['o', 'fim', 'da', 'carta'],\n",
              " ['um', 'so'],\n",
              " ['o', 'era', 'mau']]"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_text_tokenized = []\n",
        "train_text_tokenized.extend([\"<SOS>\"]+line+[\"<EOS>\"] for line in cleaned_train_text)\n",
        "val_text_tokenized = []\n",
        "val_text_tokenized.extend([\"<SOS>\"]+line+[\"<EOS>\"] for line in cleaned_val_text)\n",
        "test_text_tokenized = []\n",
        "test_text_tokenized.extend([\"<SOS>\"]+line+[\"<EOS>\"] for line in cleaned_test_text)"
      ],
      "metadata": {
        "id": "cEEo1PUjGfiN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_text_tokenized[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "ufrygCWAGs7G",
        "outputId": "a1460fd1-0ce7-4136-f94f-24ab75c74bb2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['<SOS>', '1', '<EOS>'],\n",
              " ['<SOS>', 'ministério', 'da', 'cultura', '<EOS>'],\n",
              " ['<SOS>', 'fundação', 'biblioteca', 'nacional', '<EOS>'],\n",
              " ['<SOS>', 'departamento', 'nacional', 'do', 'livro', '<EOS>'],\n",
              " ['<SOS>', 'a', 'mão', 'e', 'a', 'luva', '<EOS>'],\n",
              " ['<SOS>', 'machado', 'de', 'assis', '<EOS>'],\n",
              " ['<SOS>', 'i', '<EOS>'],\n",
              " ['<SOS>', 'o', 'fim', 'da', 'carta', '<EOS>'],\n",
              " ['<SOS>', 'um', 'so', '<EOS>'],\n",
              " ['<SOS>', 'o', 'era', 'mau', '<EOS>']]"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "palavra = ['<SOS>', 'departamento', 'nacional', 'do', 'livro', '<EOS>', '<pad>']\n",
        "teste = tokenizer(palavra, vocab)\n",
        "teste"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DST702Cdbyqu",
        "outputId": "ec3f23d7-0272-4948-bc6c-3bd9497d313a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1, 12221, 1555, 11, 265, 2, 0]"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "context_size = 9 # 9 palavras de entrada. O target é a próxima palavra"
      ],
      "metadata": {
        "id": "Iy-elI1magRR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "\n",
        "# MELHORAR CÓDIGO\n",
        "# CÓDIGO INICIAL, MAS FUNCIONA\n",
        "class MachadaoDataset(Dataset):\n",
        "    def __init__(self, vocab, context_size, data):\n",
        "        self.vocab = vocab\n",
        "        self.context_size = context_size\n",
        "        self.data = []\n",
        "\n",
        "        context_ids = []\n",
        "        target_ids = []\n",
        "\n",
        "\n",
        "        for line in data:\n",
        "            indice = 0\n",
        "\n",
        "            # em minha defesa não existe do-while no python, não que fazer do-while seja mais bonito, mas entre do-while e while-true a resposta é meio obvia\n",
        "            while True:\n",
        "\n",
        "                context = line[indice: indice + context_size]\n",
        "                target = line[indice + 1:  (indice + 1) + context_size]\n",
        "\n",
        "                if (len(context) < context_size):\n",
        "                    context = context + ([\"<PAD>\"] * (context_size - (len(context))))\n",
        "\n",
        "                if (len(target) < context_size):\n",
        "                    target = target + ([\"<PAD>\"] * (context_size - (len(target))))\n",
        "\n",
        "                indice = indice + context_size\n",
        "\n",
        "                if (\"<EOS>\" in target):\n",
        "                    break\n",
        "\n",
        "            context_ids.append(tokenizer(context, self.vocab))\n",
        "            # substitui 0 e 3 por -100 no target\n",
        "            target_token_ids = tokenizer(target, self.vocab)\n",
        "            target_token_ids = [-100 if t in (0, 3) else t for t in target_token_ids]\n",
        "            target_ids.append(target_token_ids)\n",
        "\n",
        "        self.X = torch.tensor(context_ids, dtype=torch.long)\n",
        "        self.y = torch.tensor(target_ids, dtype=torch.long)\n",
        "    def __len__(self):\n",
        "      return len(self.X)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "      return self.X[idx], self.y[idx]"
      ],
      "metadata": {
        "id": "uHTa4mEwQ2Qm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aD1CVci2zJ_J"
      },
      "outputs": [],
      "source": [
        "train_dataset = MachadaoDataset(vocab, context_size, train_text_tokenized)\n",
        "val_dataset = MachadaoDataset(vocab, context_size, val_text_tokenized)\n",
        "test_dataset = MachadaoDataset(vocab, context_size, test_text_tokenized)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset[26]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kqXATCuflCIK",
        "outputId": "3ab0953d-b079-4709-b413-be8bae06f94f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([ 1, 47,  2,  3,  3,  3,  3,  3,  3]),\n",
              " tensor([  47,    2, -100, -100, -100, -100, -100, -100, -100]))"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset[26][0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nrerpflC6Vsz",
        "outputId": "11fc9a24-38ed-432f-f680-24691f4550e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ 1, 47,  2,  3,  3,  3,  3,  3,  3])"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset[26][0].tolist()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p2j8VZ0E6WqR",
        "outputId": "4b1304a5-63e9-47ee-af9d-ee9525b7cf22"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1, 47, 2, 3, 3, 3, 3, 3, 3]"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lin1 = detokenizer(train_dataset[26][0].tolist())\n",
        "lin1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "76OVWfjllZoR",
        "outputId": "1ce43dcb-831f-4c33-ea4c-d60a938b26b7",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['<SOS>', 'há', '<EOS>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lin2 = detokenizer(train_dataset[26][1].tolist())\n",
        "lin2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VDsHeV7Um-98",
        "outputId": "22113415-e5db-4fc4-c090-566bc2a452e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['há', '<EOS>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(train_dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HZ4dnYvhlFmD",
        "outputId": "3c00d2c3-ef05-4875-9cbc-a4345ace34ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "17126"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(val_dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EiEp8tzalHe-",
        "outputId": "360a0067-71a0-4f04-e168-e5b729f3e332"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3362"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 32\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "sample = next(iter(train_loader))"
      ],
      "metadata": {
        "id": "griJE9kiuE-u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-5_-Yud0zJ_K"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math"
      ],
      "metadata": {
        "id": "za_-Vm__uhLE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# code based in https://saurabhraj5162.medium.com/day-5-lora-from-scratch-using-pytorch-ai-ml-coding-series-c28e12c39f47\n",
        "# a base do LoRA seria W_merged = W + (alpha/rank) * (B*(A*x))\n",
        "\n",
        "class LoraLinear(nn.Module):\n",
        "    def __init__(self, in_features, out_features, rank, alpha, dropout):\n",
        "        super().__init__()\n",
        "        self.rank = rank\n",
        "        self.alpha = alpha\n",
        "        self.in_features = in_features\n",
        "        self.out_features = out_features\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        # Usa o Parameter para informar que este tensor será treinavel e aparecerá nos parametros, terá gradientes atualizados e será atualizado pelo otimizador\n",
        "        # Por que A usa in_featues e B usa out_features?\n",
        "        self.A = nn.Parameter(torch.zeros(self.rank, self.in_features))\n",
        "        self.B = nn.Parameter(torch.zeros(self.out_features, self.rank))\n",
        "\n",
        "        # Coloca valores aleatórios para A segundo a distribuição de Kaiming/He\n",
        "        # Ajuda  evitar gradientes explosivos ou vanishing\n",
        "        # a = math.sqrt(5) para uma escala parecida com nn.Linear e manter a compatibilidade entre pesos LoRA e pesos padrão\n",
        "        nn.init.kaiming_uniform_(self.A, a = math.sqrt(5))\n",
        "        # B é inicializada com 0\n",
        "        nn.init.zeros_(self.B)\n",
        "\n",
        "        self.weight = torch.empty(self.out_features, self.in_features)\n",
        "\n",
        "\n",
        "    def forward(self, original_weight, embedding):\n",
        "        # Pega os pesos que da camada com X\n",
        "        # usa nn.functional.linear para pegar os pesos, ao inves de criar uma nn.Linear\n",
        "        # Se assemelha a x * Wt + b\n",
        "        # Não faz sentido pegar apenas o peso original, é necessário usar a saída que seria do Linear\n",
        "        self.weight = original_weight.data\n",
        "\n",
        "        weight = F.linear(embedding, self.weight)\n",
        "\n",
        "        scaling = self.alpha/self.rank\n",
        "\n",
        "        dropout = self.dropout(embedding)\n",
        "\n",
        "        # usamos matmul, por que é a multiplicação classica de matrizes linhaXcoluna\n",
        "        A_weight = torch.matmul(dropout, self.A.t())\n",
        "        BA = torch.matmul(A_weight, self.B.t())\n",
        "\n",
        "        # (alpha/rank) * (BAx)\n",
        "        scaled_lora = scaling*BA\n",
        "\n",
        "        # Soma a camada LoRA calculada aos pesos do modelo\n",
        "        lora = weight + scaled_lora\n",
        "\n",
        "        return lora\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "arC8Ot6S589S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AttentionHead(torch.nn.Module):\n",
        "    def __init__(self, embedding_dim, head_dim, context_size, rank, alpha, dropout=0.3):\n",
        "        super().__init__()\n",
        "\n",
        "        # embedding_dim é o tamanho total do meu embedding\n",
        "        # head_dim é meu embedding/head_count, ou seja, o tamanho do meu embeedings dividio pela quantidade de cabeças de atenção\n",
        "        # essa divisão ajuda na hora do calulo da matriz\n",
        "        self.Wq = nn.Linear(embedding_dim, head_dim, bias=False)\n",
        "        self.Wk = nn.Linear(embedding_dim, head_dim, bias=False)\n",
        "        self.Wv = nn.Linear(embedding_dim, head_dim, bias=False)\n",
        "\n",
        "        self.Wq_lora = LoraLinear(self.Wq.in_features, self.Wq.out_features, rank, alpha, dropout)\n",
        "        self.Wv_lora = LoraLinear(self.Wv.in_features, self.Wv.out_features, rank, alpha, dropout)\n",
        "\n",
        "        # torch.tril cria uma matrix de 1s onde apenas a diagonal inferior receberá valores, o resto será 0s\n",
        "        # self.register_buffer registra o elemento como parte da classe, mas não como parametro treinavel, não srecebe gradientte e nem será atualizado com otimizador\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(context_size, context_size)))\n",
        "\n",
        "        # Dropout desliga alguns neuronios para evitar overfiting e tornar o modelo mais generalista\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "\n",
        "    def forward(self, embedding, has_lora=False):\n",
        "        batch_size, context_size, embed_dim = embedding.size() # (batch_size, seq_len, emb_dim)\n",
        "\n",
        "        if(has_lora):\n",
        "            Q = self.Wq_lora(self.Wq.weight, embedding)\n",
        "            V = self.Wv_lora(self.Wv.weight, embedding)\n",
        "        else:\n",
        "            Q = self.Wq(embedding) # (batch_size, seq_len, head_dim)\n",
        "            V = self.Wv(embedding)\n",
        "\n",
        "        K = self.Wk(embedding)\n",
        "\n",
        "        scores = torch.matmul(Q, K.transpose(-2, -1)) / (K.size(-1) ** 0.5)  # (batch, seq_len, seq_len)\n",
        "\n",
        "        # Calculo a mascara causal\n",
        "        # Tudo no triangulo de cima recebe -inf, e fica \"desligado\" para os calculos\n",
        "        causal_mask = scores.masked_fill(self.tril[:context_size, :context_size] == 0, float('-inf')) # (batch, seq_len, seq_len)\n",
        "\n",
        "        probs = F.softmax(causal_mask, dim=-1)  # (batch, seq_len, seq_len)\n",
        "\n",
        "        drop = self.dropout(probs)\n",
        "\n",
        "        outputs = torch.matmul(drop, V)  # (batch, seq_len, head_dim)\n",
        "\n",
        "        return outputs"
      ],
      "metadata": {
        "id": "JowVJJ2AuZc6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(torch.nn.Module):\n",
        "    def __init__(self, embedding_dim, head_number, context_size, rank, alpha, dropout=0.3):\n",
        "        super().__init__()\n",
        "\n",
        "        # Divide pelo numero de cabeças, para que na hora da concatenação, não dê erro de shape\n",
        "        self.emb_dim = embedding_dim // head_number\n",
        "\n",
        "        # Cria uma lista com a quantidade de cabeças de atenção que serão uilizadas\n",
        "        self.heads = nn.ModuleList(\n",
        "            [AttentionHead(embedding_dim, self.emb_dim, context_size, rank, alpha) for _ in range(head_number)]\n",
        "        )\n",
        "\n",
        "        # camada de projeção e dropout\n",
        "        self.Wout = nn.Linear(embedding_dim, embedding_dim)\n",
        "        self.Wout_lora = LoraLinear(self.Wout.in_features, self.Wout.out_features, rank, alpha, dropout)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, embedding, has_lora=False):\n",
        "        # faz a concatenação do retorno cabeça de atenção\n",
        "        # por causa do dim=-1, a concatenação será apenas na ultima dimensão\n",
        "        # o shape retornado por cada cabeça é (batch, seq_len, embedding_dim // head_number)\n",
        "        # ao fazer a concatenação o shape final será (batch, seq_len, embedding_dim)\n",
        "        concat = torch.cat([head(embedding, has_lora) for head in self.heads], dim=-1)\n",
        "\n",
        "        if (has_lora):\n",
        "            w_out = self.Wout_lora(self.Wout.weight, concat)\n",
        "        else:\n",
        "            w_out = self.Wout(concat)\n",
        "\n",
        "        drop = self.dropout(w_out)\n",
        "        return drop"
      ],
      "metadata": {
        "id": "5DXNUNiBcYn6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerBlock(torch.nn.Module):\n",
        "    def __init__(self, embedding_dim, head_number, context_size, rank, alpha, dropout=0.3):\n",
        "        super().__init__()\n",
        "\n",
        "        self.multi_head = MultiHeadAttention(embedding_dim, head_number, context_size, rank, alpha)\n",
        "\n",
        "        # MLP\n",
        "        self.linear1 = nn.Linear(embedding_dim, 4 * embedding_dim)\n",
        "        self.linear1_lora = LoraLinear(self.linear1.in_features, self.linear1.out_features, rank, alpha, dropout)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.linear2 = nn.Linear(4 * embedding_dim, embedding_dim)\n",
        "        self.linear2_lora = LoraLinear(self.linear2.in_features, self.linear2.out_features, rank, alpha, dropout)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        # Camadas de normalização\n",
        "        self.layer_norm1 = nn.LayerNorm(embedding_dim)\n",
        "        self.layer_norm2 = nn.LayerNorm(embedding_dim)\n",
        "\n",
        "    def forward(self, embedding, has_lora=False):\n",
        "\n",
        "        layer_norm1 = self.layer_norm1(embedding) # (batch_size, context_size, embedding_dim)\n",
        "        multi_head = self.multi_head(layer_norm1, has_lora) # (batch_size, context_size, embedding_dim)\n",
        "        embedding = embedding + multi_head # (batch_size, context_size, embedding_dim)\n",
        "\n",
        "\n",
        "        layer_norm2 = self.layer_norm2(embedding) # (batch_size, context_size, embedding_dim)\n",
        "\n",
        "        if (has_lora):\n",
        "            linear1 = self.linear1_lora(self.linear1.weight, layer_norm2) # (batch_size, context_size, 4 * embedding_dim)\n",
        "            relu = self.relu(linear1)\n",
        "            linear2 = self.linear2_lora(self.linear2.weight, relu)\n",
        "        else:\n",
        "            linear1 = self.linear1(layer_norm2) # (batch_size, context_size, 4 * embedding_dim)\n",
        "            relu = self.relu(linear1)\n",
        "            linear2 = self.linear2(relu)\n",
        "\n",
        "\n",
        "        embedding = embedding + linear2\n",
        "\n",
        "        return embedding"
      ],
      "metadata": {
        "id": "G9YstYUjepJ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I2qKG9YczJ_K"
      },
      "outputs": [],
      "source": [
        "class LanguageModel(torch.nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, head_number, context_size, layer_number, rank, alpha, dropout=0.3):\n",
        "        super().__init__()\n",
        "        \"\"\"TODO: implementar o modelo de linguagem\"\"\"\n",
        "        # Camada de embeddings\n",
        "        # vocab_size = tamanho do vocabulario\n",
        "        # embedding_dim = representação vetorial de cada palavra\n",
        "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
        "\n",
        "        #Cria um vetor com o tamanho do vetor de embedding e uma quantidade de linhas\n",
        "        self.pos_embeddings = nn.Embedding(context_size, embedding_dim)\n",
        "\n",
        "\n",
        "        # O * desempacota e envia os objetos de forma separada\n",
        "        # Sequential é conteiner do python que encaixa modulos em sequencia\n",
        "        self.transformer_blocks = nn.ModuleList(\n",
        "                                        [TransformerBlock(embedding_dim, head_number, context_size, rank, alpha)\n",
        "                                                                  for _ in range(layer_number)]\n",
        "                                                )\n",
        "\n",
        "        self.layer_norm = nn.LayerNorm(embedding_dim)\n",
        "        self.linear_out = nn.Linear(embedding_dim, vocab_size)\n",
        "\n",
        "    def forward(self, inputs, targets=None, has_lora=False):\n",
        "\n",
        "        batch_size, seq_len = inputs.size()\n",
        "\n",
        "        embed = self.embeddings(inputs) # (batch_size, context_size, embedding_dim)\n",
        "\n",
        "        positions = torch.arange(seq_len, device=inputs.device).unsqueeze(0).expand(batch_size, seq_len)\n",
        "        pos_emb = self.pos_embeddings(positions) # (batch_size, context_size, embedding_dim)\n",
        "\n",
        "        embeddings = embed + pos_emb # (batch_size, context_size, embedding_dim)\n",
        "\n",
        "        emb_transformer = embeddings\n",
        "        for block in self.transformer_blocks:\n",
        "            emb_transformer = block(emb_transformer, has_lora)\n",
        "        transformer_block = emb_transformer\n",
        "\n",
        "        layer_norm = self.layer_norm(transformer_block) # (B,T,C)\n",
        "\n",
        "        logits = self.linear_out(layer_norm)\n",
        "\n",
        "        return logits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7yjQ1KXOzJ_K"
      },
      "outputs": [],
      "source": [
        "model = LanguageModel(vocab_size, 128, 4, context_size, 4, rank=4, alpha=32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xmsD59TfzJ_K"
      },
      "outputs": [],
      "source": [
        "# sample = next(iter(train_loader))\n",
        "input = sample[0]\n",
        "target = sample[1]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "MqX5wdyg70VJ",
        "outputId": "578ab6ad-8e2e-4b7b-83cb-c218f9e2bf24"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[    1,   112,  6757,     2,     3,     3,     3,     3,     3],\n",
              "        [    1,  8243,     2,     3,     3,     3,     3,     3,     3],\n",
              "        [    1,  2291,     2,     3,     3,     3,     3,     3,     3],\n",
              "        [    1,     8, 12093,     2,     3,     3,     3,     3,     3],\n",
              "        [    1,   961,     2,     3,     3,     3,     3,     3,     3],\n",
              "        [    1,  3657,     2,     3,     3,     3,     3,     3,     3],\n",
              "        [    1,  1554,   683,     2,     3,     3,     3,     3,     3],\n",
              "        [    1,  1521,     2,     3,     3,     3,     3,     3,     3],\n",
              "        [    1,     9,    33,  9479,    55,     5,   292,  2794,  5488],\n",
              "        [ 1057,    11,    88,     5,    37,    31,     2,     3,     3],\n",
              "        [    1,  1763,     2,     3,     3,     3,     3,     3,     3],\n",
              "        [    1,  8444,     2,     3,     3,     3,     3,     3,     3],\n",
              "        [ 1006,  3232,     4,   152,  3655,     2,     3,     3,     3],\n",
              "        [    1,  2263, 12397,     2,     3,     3,     3,     3,     3],\n",
              "        [    1,     4,   349,  6050,     8,     5,    24,   272,  7635],\n",
              "        [    1,     9,    33,   917,     8,     2,     3,     3,     3],\n",
              "        [ 4871,    36,   109,  1307,   139,   284,   142,     2,     3],\n",
              "        [    1,     7,     2,     3,     3,     3,     3,     3,     3],\n",
              "        [    1,  1394,     2,     3,     3,     3,     3,     3,     3],\n",
              "        [    1,  5013,  5395,    73,   362,    63,    34,    25,     2],\n",
              "        [    1,   402,  1462,  6883,     2,     3,     3,     3,     3],\n",
              "        [    1,   112, 14229,     2,     3,     3,     3,     3,     3],\n",
              "        [    1,  1036,     2,     3,     3,     3,     3,     3,     3],\n",
              "        [    1,     4,  2007,     2,     3,     3,     3,     3,     3],\n",
              "        [    1,  3050,     2,     3,     3,     3,     3,     3,     3],\n",
              "        [    1,   112, 13730,     2,     3,     3,     3,     3,     3],\n",
              "        [    1, 13155,     2,     3,     3,     3,     3,     3,     3],\n",
              "        [    1,     8,  3013,     2,     3,     3,     3,     3,     3],\n",
              "        [    1,  1262,     2,     3,     3,     3,     3,     3,     3],\n",
              "        [    1,     2,     3,     3,     3,     3,     3,     3,     3],\n",
              "        [   72,  2384,    15,     4,   179,    11,     2,     3,     3],\n",
              "        [    1, 19916,     9,  1789,     4,  2580,   674,     2,     3]])"
            ]
          },
          "metadata": {},
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HGbJcT5KzJ_K",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "logits = model(input, target, False)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"for name, p in model_teste.named_parameters():\n",
        "    if \"A\" in name or \"B\" in name:\n",
        "        p.requires_grad = True\n",
        "    else:\n",
        "        p.requires_grad = False\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "G25TmjSYEcbb",
        "outputId": "2cd007e9-7759-4dae-f0ae-a52282389233"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'for name, p in model_teste.named_parameters():\\n    if \"A\" in name or \"B\" in name:\\n        p.requires_grad = True\\n    else:\\n        p.requires_grad = False'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#logits = model_teste(input, target, True)"
      ],
      "metadata": {
        "id": "Ut7gOKPgEgRW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "logits.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R07Zo8EA65c1",
        "outputId": "ed806e1f-73e8-4583-e0c4-03343e0b6af0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([32, 9, 20004])"
            ]
          },
          "metadata": {},
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "um0lR4mNzJ_K",
        "outputId": "8ec50ab0-257b-4232-9231-b105502b96ee",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[2, 6, 6,  ..., 7, 3, 1],\n",
              "        [5, 6, 1,  ..., 7, 1, 1],\n",
              "        [5, 6, 1,  ..., 7, 0, 7],\n",
              "        ...,\n",
              "        [5, 6, 1,  ..., 7, 1, 3],\n",
              "        [0, 6, 1,  ..., 7, 0, 5],\n",
              "        [6, 3, 6,  ..., 7, 2, 3]])"
            ]
          },
          "metadata": {},
          "execution_count": 71
        }
      ],
      "source": [
        "logits.argmax(dim=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "la-b-f8jzJ_L",
        "outputId": "b0257ec8-1e28-4aa5-98ad-6d02d8576016",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[  112,  6757,     2,  -100,  -100,  -100,  -100,  -100,  -100],\n",
              "        [ 8243,     2,  -100,  -100,  -100,  -100,  -100,  -100,  -100],\n",
              "        [ 2291,     2,  -100,  -100,  -100,  -100,  -100,  -100,  -100],\n",
              "        [    8, 12093,     2,  -100,  -100,  -100,  -100,  -100,  -100],\n",
              "        [  961,     2,  -100,  -100,  -100,  -100,  -100,  -100,  -100],\n",
              "        [ 3657,     2,  -100,  -100,  -100,  -100,  -100,  -100,  -100],\n",
              "        [ 1554,   683,     2,  -100,  -100,  -100,  -100,  -100,  -100],\n",
              "        [ 1521,     2,  -100,  -100,  -100,  -100,  -100,  -100,  -100],\n",
              "        [    9,    33,  9479,    55,     5,   292,  2794,  5488,     2],\n",
              "        [   11,    88,     5,    37,    31,     2,  -100,  -100,  -100],\n",
              "        [ 1763,     2,  -100,  -100,  -100,  -100,  -100,  -100,  -100],\n",
              "        [ 8444,     2,  -100,  -100,  -100,  -100,  -100,  -100,  -100],\n",
              "        [ 3232,     4,   152,  3655,     2,  -100,  -100,  -100,  -100],\n",
              "        [ 2263, 12397,     2,  -100,  -100,  -100,  -100,  -100,  -100],\n",
              "        [    4,   349,  6050,     8,     5,    24,   272,  7635,     2],\n",
              "        [    9,    33,   917,     8,     2,  -100,  -100,  -100,  -100],\n",
              "        [   36,   109,  1307,   139,   284,   142,     2,  -100,  -100],\n",
              "        [    7,     2,  -100,  -100,  -100,  -100,  -100,  -100,  -100],\n",
              "        [ 1394,     2,  -100,  -100,  -100,  -100,  -100,  -100,  -100],\n",
              "        [ 5013,  5395,    73,   362,    63,    34,    25,     2,  -100],\n",
              "        [  402,  1462,  6883,     2,  -100,  -100,  -100,  -100,  -100],\n",
              "        [  112, 14229,     2,  -100,  -100,  -100,  -100,  -100,  -100],\n",
              "        [ 1036,     2,  -100,  -100,  -100,  -100,  -100,  -100,  -100],\n",
              "        [    4,  2007,     2,  -100,  -100,  -100,  -100,  -100,  -100],\n",
              "        [ 3050,     2,  -100,  -100,  -100,  -100,  -100,  -100,  -100],\n",
              "        [  112, 13730,     2,  -100,  -100,  -100,  -100,  -100,  -100],\n",
              "        [13155,     2,  -100,  -100,  -100,  -100,  -100,  -100,  -100],\n",
              "        [    8,  3013,     2,  -100,  -100,  -100,  -100,  -100,  -100],\n",
              "        [ 1262,     2,  -100,  -100,  -100,  -100,  -100,  -100,  -100],\n",
              "        [    2,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100],\n",
              "        [ 2384,    15,     4,   179,    11,     2,  -100,  -100,  -100],\n",
              "        [19916,     9,  1789,     4,  2580,   674,     2,  -100,  -100]])"
            ]
          },
          "metadata": {},
          "execution_count": 72
        }
      ],
      "source": [
        "target"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "non_trainable_params = sum(p.numel() for p in model.parameters() if not p.requires_grad)\n",
        "\n",
        "print(f\"Parâmetros treináveis: {trainable_params:,}\")\n",
        "print(f\"Parâmetros não-treináveis: {non_trainable_params:,}\")\n",
        "print(f\"Total: {trainable_params + non_trainable_params:,}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R3TxCkMoSvi6",
        "outputId": "3450f7a5-fd29-438f-8ae1-e71b2092e223"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parâmetros treináveis: 5,979,044\n",
            "Parâmetros não-treináveis: 0\n",
            "Total: 5,979,044\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UngUhyu7zJ_L"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wntaV50nzJ_L",
        "outputId": "ead22194-0fe5-4180-b3c7-77c6c1c59792"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 74
        }
      ],
      "source": [
        "# Verifica se há uma GPU disponível e define o dispositivo para GPU se possível, caso contrário, usa a CPU\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "def MeasurePerplexity(model, data_loader, criterion, device, has_lora=False):\n",
        "    model.eval()\n",
        "\n",
        "    total_loss = 0.0\n",
        "    total_samples = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in data_loader:\n",
        "          inputs = inputs.to(device)\n",
        "          labels = labels.to(device)\n",
        "\n",
        "          outputs = model(inputs, labels, has_lora)\n",
        "\n",
        "          batch_size, seq_len, vocab_size = outputs.shape\n",
        "          outputs = outputs.view(batch_size*seq_len, vocab_size)  # (batch_size*seq_len, vocab_size)\n",
        "          labels = labels.view(batch_size*seq_len)   # (batch_size*seq_len)\n",
        "\n",
        "          # SUGESTÃO PARA IGNORAR OS UNK loss = F.cross_entropy(logits, targets, ingnore_index=[...])\n",
        "          # colocar o valor token <unk> no ignore_index\n",
        "          loss = criterion(outputs, labels)\n",
        "\n",
        "          batch_size = inputs.size(0)\n",
        "          total_loss += loss.detach() * batch_size\n",
        "          total_samples += batch_size\n",
        "\n",
        "        avg_loss = total_loss / total_samples\n",
        "        perplexity = torch.exp(avg_loss).item()\n",
        "\n",
        "        return avg_loss.item(), perplexity"
      ],
      "metadata": {
        "id": "K6nmPDJi3QCN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import random\n",
        "import torch.optim as optim\n",
        "\n",
        "random.seed(42)\n",
        "epochs = 10\n",
        "lr = 5e-5\n",
        "\n",
        "model = model.to(device)\n",
        "# CrossEntropy quantifica o quão bem as predições do modelo se igualam aos resultado reais\n",
        "# Quanto maior confiança o modelo tem em predizer corretamente, menor a loss\n",
        "# Quanto maior a confiança do modelo me predizer errado, maior a loss\n",
        "#criterion = nn.CrossEntropyLoss(ignore_index=vocab[\"<PAD>\"])\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=0.01)"
      ],
      "metadata": {
        "id": "f0Rk86sn5Xz3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"loss, perp = MeasurePerplexity(model, val_loader, criterion, device)\n",
        "\n",
        "print(\"Dados do modelo antes do treinamento\")\n",
        "print(\"Perplexity: \", perp)\n",
        "print(\"Loss: \", loss)\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "-5tCemRQ5cE7",
        "outputId": "b8223cc3-5c8a-43c4-df40-0aac83ff492e",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'loss, perp = MeasurePerplexity(model, val_loader, criterion, device)\\n\\nprint(\"Dados do modelo antes do treinamento\")\\nprint(\"Perplexity: \", perp)\\nprint(\"Loss: \", loss)'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Treinamento com modelo sem LoRA - 70% do dataset"
      ],
      "metadata": {
        "id": "ffF1XM4dyUIb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 10\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    start_time = time.time()  # Start time of the epoch\n",
        "    epoch_loss = 0\n",
        "    total_samples = 0\n",
        "    model.train()\n",
        "    for inputs, labels in train_loader:\n",
        "\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(inputs, labels, False)\n",
        "\n",
        "        batch_size, seq_len, vocab_size = outputs.shape\n",
        "        outputs = outputs.view(batch_size*seq_len, vocab_size)  # (batch_size*seq_len, vocab_size)\n",
        "        labels = labels.view(batch_size*seq_len)   # (batch_size*seq_len)\n",
        "\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backward\n",
        "        # Zerando os gradiente calculados\n",
        "        optimizer.zero_grad()\n",
        "        # Fazendo calculo de backpropagation\n",
        "        loss.backward()\n",
        "        # Atualizando os pesos do modelo\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_loss+= loss.detach() * inputs.size(0)\n",
        "        total_samples += inputs.size(0)\n",
        "\n",
        "    end_time = time.time()  # End time of the epoch\n",
        "    epoch_duration = end_time - start_time  # Duration of epoch\n",
        "    avg_epoch_loss = epoch_loss / total_samples\n",
        "    train_perplexity = torch.exp(avg_epoch_loss).item()\n",
        "\n",
        "    print('Training Data:')\n",
        "    print(f'Epoch [{epoch+1}/{epochs}], \\\n",
        "            Loss: {avg_epoch_loss.item():.4f},\\\n",
        "            Perplexity: {train_perplexity:.4f},\\\n",
        "            Elapsed Time: {epoch_duration:.2f} sec')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "cM1RLHhbhWIf",
        "outputId": "37876f37-1dfc-42a1-8168-26ad2356df21"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Data:\n",
            "Epoch [1/10],             Loss: 7.1457,            Perplexity: 1268.5948,            Elapsed Time: 13.03 sec\n",
            "Training Data:\n",
            "Epoch [2/10],             Loss: 5.7257,            Perplexity: 306.6556,            Elapsed Time: 13.05 sec\n",
            "Training Data:\n",
            "Epoch [3/10],             Loss: 5.4689,            Perplexity: 237.2003,            Elapsed Time: 14.31 sec\n",
            "Training Data:\n",
            "Epoch [4/10],             Loss: 5.3130,            Perplexity: 202.9600,            Elapsed Time: 12.78 sec\n",
            "Training Data:\n",
            "Epoch [5/10],             Loss: 5.1909,            Perplexity: 179.6364,            Elapsed Time: 12.90 sec\n",
            "Training Data:\n",
            "Epoch [6/10],             Loss: 5.0804,            Perplexity: 160.8309,            Elapsed Time: 12.91 sec\n",
            "Training Data:\n",
            "Epoch [7/10],             Loss: 4.9848,            Perplexity: 146.1788,            Elapsed Time: 13.03 sec\n",
            "Training Data:\n",
            "Epoch [8/10],             Loss: 4.8945,            Perplexity: 133.5466,            Elapsed Time: 12.78 sec\n",
            "Training Data:\n",
            "Epoch [9/10],             Loss: 4.8129,            Perplexity: 123.0854,            Elapsed Time: 12.69 sec\n",
            "Training Data:\n",
            "Epoch [10/10],             Loss: 4.7379,            Perplexity: 114.1887,            Elapsed Time: 12.79 sec\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Treinamento do modelo congelando os pesos e treinando somente A e B - 20% do dataset"
      ],
      "metadata": {
        "id": "5W5OsaFvyafj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for name, p in model.named_parameters():\n",
        "    if \"A\" in name or \"B\" in name:\n",
        "        p.requires_grad = True\n",
        "    else:\n",
        "        p.requires_grad = False"
      ],
      "metadata": {
        "id": "daPUYlrihhUB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=0.01)"
      ],
      "metadata": {
        "id": "XO4ZEvLIrcVL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 10\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    start_time = time.time()  # Start time of the epoch\n",
        "    epoch_loss = 0\n",
        "    total_samples = 0\n",
        "    model.train()\n",
        "    for inputs, labels in val_loader:\n",
        "\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(inputs, labels, True)\n",
        "\n",
        "        batch_size, seq_len, vocab_size = outputs.shape\n",
        "        outputs = outputs.view(batch_size*seq_len, vocab_size)  # (batch_size*seq_len, vocab_size)\n",
        "        labels = labels.view(batch_size*seq_len)   # (batch_size*seq_len)\n",
        "\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backward\n",
        "        # Zerando os gradiente calculados\n",
        "        optimizer.zero_grad()\n",
        "        # Fazendo calculo de backpropagation\n",
        "        loss.backward()\n",
        "        # Atualizando os pesos do modelo\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_loss+= loss.detach() * inputs.size(0)\n",
        "        total_samples += inputs.size(0)\n",
        "\n",
        "    end_time = time.time()  # End time of the epoch\n",
        "    epoch_duration = end_time - start_time  # Duration of epoch\n",
        "    avg_epoch_loss = epoch_loss / total_samples\n",
        "    train_perplexity = torch.exp(avg_epoch_loss).item()\n",
        "\n",
        "    print('Training Data:')\n",
        "    print(f'Epoch [{epoch+1}/{epochs}], \\\n",
        "            Loss: {avg_epoch_loss.item():.4f},\\\n",
        "            Perplexity: {train_perplexity:.4f},\\\n",
        "            Elapsed Time: {epoch_duration:.2f} sec')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "EBpcvj3khxzi",
        "outputId": "ced4ab3e-24b3-48c6-8705-884996f7a01d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Data:\n",
            "Epoch [1/10],             Loss: 5.4997,            Perplexity: 244.6268,            Elapsed Time: 3.68 sec\n",
            "Training Data:\n",
            "Epoch [2/10],             Loss: 5.4463,            Perplexity: 231.8893,            Elapsed Time: 4.26 sec\n",
            "Training Data:\n",
            "Epoch [3/10],             Loss: 5.3926,            Perplexity: 219.7722,            Elapsed Time: 3.62 sec\n",
            "Training Data:\n",
            "Epoch [4/10],             Loss: 5.3695,            Perplexity: 214.7598,            Elapsed Time: 3.62 sec\n",
            "Training Data:\n",
            "Epoch [5/10],             Loss: 5.3493,            Perplexity: 210.4600,            Elapsed Time: 4.28 sec\n",
            "Training Data:\n",
            "Epoch [6/10],             Loss: 5.3353,            Perplexity: 207.5376,            Elapsed Time: 3.59 sec\n",
            "Training Data:\n",
            "Epoch [7/10],             Loss: 5.3258,            Perplexity: 205.5816,            Elapsed Time: 3.63 sec\n",
            "Training Data:\n",
            "Epoch [8/10],             Loss: 5.3147,            Perplexity: 203.3059,            Elapsed Time: 4.80 sec\n",
            "Training Data:\n",
            "Epoch [9/10],             Loss: 5.3158,            Perplexity: 203.5329,            Elapsed Time: 3.62 sec\n",
            "Training Data:\n",
            "Epoch [10/10],             Loss: 5.3017,            Perplexity: 200.6857,            Elapsed Time: 3.63 sec\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Teste do modelo - 10% do dataset"
      ],
      "metadata": {
        "id": "YUDFR0sUyh7s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss, perp = MeasurePerplexity(model, test_loader, criterion, device, True)\n",
        "\n",
        "print(\"Validation Data:\")\n",
        "print(\"Perplexity: \", perp)\n",
        "print(\"Loss: \", loss)"
      ],
      "metadata": {
        "id": "krTKSosgiJ9v",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "04990783-f2b8-4626-f75a-13aa154fd349"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Data:\n",
            "Perplexity:  147.2348175048828\n",
            "Loss:  4.992028713226318\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A1zhxVqfzJ_M"
      },
      "source": [
        "## Exemplo de uso"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def sample_next_token(logits, top_k=50, top_p=0.9, temperature=1.0):\n",
        "    # Ajusta a \"temperatura\" para controlar a aleatoriedade\n",
        "    # temperatura = 1 sugere um comportamento normal\n",
        "    # Se dividir por um valor < 1 os logits ficam maiores, fazendo com que os maiores se tornem mais expressivos\n",
        "    # se dividir por um valor > 1, penalizo os logits menores, que ficaram menores ainda\n",
        "    logits = logits / temperature\n",
        "    # converte me probabilidades não negativas\n",
        "    probs = F.softmax(logits, dim=-1)\n",
        "\n",
        "    # top_k representa a quantidade de logits mais representattivos que será uttilizada\n",
        "    if top_k > 0:\n",
        "        # retorna os os valores mais altos e seus ids\n",
        "        topk_probs, topk_idx = torch.topk(probs, top_k)\n",
        "        # cria uma mascara booleana\n",
        "        mask = torch.ones_like(probs, dtype=torch.bool)\n",
        "        # pcoloca False nos indidce que não fazem parte da seleção top_k\n",
        "        mask.scatter_(0, topk_idx, False)\n",
        "        # Zera as probs de todos os ids fora das prob de top_k\n",
        "        probs = probs.masked_fill(mask, 0)\n",
        "\n",
        "    # ----- TOP-P (nucleus) -----\n",
        "    if top_p < 1.0:\n",
        "        # ordena os topk do maior para o menor\n",
        "        sorted_probs, sorted_idx = torch.sort(probs, descending=True)\n",
        "        cumulative_probs = torch.cumsum(sorted_probs, dim=-1)\n",
        "        # marca o elemento que soma seja mairo que top_p\n",
        "        mask = cumulative_probs > top_p\n",
        "        # este código é para marcar todos os valores q são maiores que o top_p, o primeiro q ultrapassou, fica na lista\n",
        "        mask[1:] = mask[:-1].clone()  # shift\n",
        "        mask[0] = False\n",
        "        # Zera a problabilidade de todos os outros\n",
        "        probs = probs.clone()\n",
        "        # volta o probs a oderm normal\n",
        "        probs.scatter_(0, sorted_idx[mask], 0)\n",
        "\n",
        "    # normaliza de novo para q soma de 1\n",
        "    probs = probs / probs.sum()\n",
        "    # Amostra uma palavra do vetor de probabilidades\n",
        "    next_token = torch.multinomial(probs, 1)\n",
        "    return next_token.item()\n"
      ],
      "metadata": {
        "id": "L48NWBzdO47Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3PExkoWOzJ_M"
      },
      "outputs": [],
      "source": [
        "def generate_text(text, max_length, context_size):\n",
        "    \"\"\"TODO: implemente a função para gerar texto até atingir o max_length\"\"\"\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    words = text.split()\n",
        "    words_ids = tokenizer(words[len(words) - context_size : len(words)], vocab)\n",
        "\n",
        "\n",
        "    # Verifica se o texto tem a quantidade minima de palavras para a execução do modelo\n",
        "    with torch.no_grad():\n",
        "        # Continua executando ate atingir o maximo de palavras\n",
        "        while len(words_ids) < max_length:\n",
        "            # Caso a sentenção não tenha o minimo de palavras de contexto, não faz a execução\n",
        "            if len(words_ids) < context_size:\n",
        "                break\n",
        "\n",
        "            work_id = words_ids[len(words_ids) - context_size : len(words_ids)]\n",
        "\n",
        "            words_tensor = torch.tensor(work_id, dtype=torch.long).unsqueeze(0).to(device)\n",
        "\n",
        "            output = model(words_tensor)\n",
        "\n",
        "            last_logits = output[:, -1, :] # (1, vocab_size)\n",
        "            last_logits = logits[0, -1] # (vocab_size)\n",
        "\n",
        "            #prob_word = sample_next_word(last_logits[0], 1.0, 50)\n",
        "            prob_word = sample_next_token(last_logits, top_k=50, top_p=0.9, temperature=0.8)\n",
        "\n",
        "            if prob_word == vocab[\"<EOS>\"]:\n",
        "                break  # fim da sequência\n",
        "\n",
        "            words_ids.append(prob_word)\n",
        "\n",
        "\n",
        "    words = detokenizer(words_ids)\n",
        "\n",
        "    return ' '.join(words)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "context = 9\n",
        "max_length= 100\n",
        "text = \"<SOS> valia a pena admirar como eles comunicavam a\"\n",
        "retorno = generate_text(text, max_length, context)\n",
        "print(retorno)"
      ],
      "metadata": {
        "id": "Lk9OZahBBc9h",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "150fbbcb-ba4b-4b6a-b6d6-49beac3ee757"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<SOS> valia a pena admirar como eles comunicavam a chupado cuidavam franziu seguro patimau empuxar gênios anacoreta involuntário anacoreta perversa franziu mon fatídico mem votou franziu humanas despregadas teremos estejam maliciosamente estejam caboclo maliciosamente trajando versões suceder fatídico batem votou teremos lâmpada fatídico versões coberto alento seguro agudeza perversa incumbido suceder seguro versões mandam pedidos empuxar seguro perguntei confrades infelicidade estejam gênios maliciosamente perguntei coberto lâmpada lâmpada franziu caboclo caboclo lâmpada agudeza perversa fatídico lisa mandam lâmpada cinzenta perguntei empuxar estejam ombro incumbido achavaa chupado mon pernas agudeza os involuntário gênios patimau cabo teremos chupado cuidavam pedidos duvidou duvidou votou\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GCtTaqqhDkEG"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}